<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>ML_Workflows • icardaFIGSr</title>
<script src="../deps/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="../deps/bootstrap-5.3.1/bootstrap.min.css" rel="stylesheet">
<script src="../deps/bootstrap-5.3.1/bootstrap.bundle.min.js"></script><link href="../deps/font-awesome-6.5.2/css/all.min.css" rel="stylesheet">
<link href="../deps/font-awesome-6.5.2/css/v4-shims.min.css" rel="stylesheet">
<script src="../deps/headroom-0.11.0/headroom.min.js"></script><script src="../deps/headroom-0.11.0/jQuery.headroom.min.js"></script><script src="../deps/bootstrap-toc-1.0.1/bootstrap-toc.min.js"></script><script src="../deps/clipboard.js-2.0.11/clipboard.min.js"></script><script src="../deps/search-1.0.0/autocomplete.jquery.min.js"></script><script src="../deps/search-1.0.0/fuse.min.js"></script><script src="../deps/search-1.0.0/mark.min.js"></script><!-- pkgdown --><script src="../pkgdown.js"></script><meta property="og:title" content="ML_Workflows">
</head>
<body>
    <a href="#main" class="visually-hidden-focusable">Skip to contents</a>


    <nav class="navbar navbar-expand-lg fixed-top bg-light" data-bs-theme="light" aria-label="Site navigation"><div class="container">

    <a class="navbar-brand me-2" href="../index.html">icardaFIGSr</a>

    <small class="nav-text text-muted me-auto" data-bs-toggle="tooltip" data-bs-placement="bottom" title="">1.0.2</small>


    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar" class="collapse navbar-collapse ms-3">
      <ul class="navbar-nav me-auto">
<li class="nav-item"><a class="nav-link" href="../reference/index.html">Reference</a></li>
<li class="active nav-item dropdown">
  <button class="nav-link dropdown-toggle" type="button" id="dropdown-articles" data-bs-toggle="dropdown" aria-expanded="false" aria-haspopup="true">Articles</button>
  <ul class="dropdown-menu" aria-labelledby="dropdown-articles">
<li><a class="dropdown-item" href="../articles/CropData.html">CropData</a></li>
    <li><a class="dropdown-item" href="../articles/ML_Workflows.html">ML_Workflows</a></li>
    <li><a class="dropdown-item" href="../articles/Sites_climate.html">Sites_climate</a></li>
  </ul>
</li>
      </ul>
<ul class="navbar-nav">
<li class="nav-item"><form class="form-inline" role="search">
 <input class="form-control" type="search" name="search-input" id="search-input" autocomplete="off" aria-label="Search site" placeholder="Search for" data-search-index="../search.json">
</form></li>
      </ul>
</div>


  </div>
</nav><div class="container template-article">




<div class="row">
  <main id="main" class="col-md-9"><div class="page-header">

      <h1>ML_Workflows</h1>
                        <h4 data-toc-skip class="author">IcardaFIGSr
Team</h4>
            
            <h4 data-toc-skip class="date">2025-01-03</h4>
      

      <div class="d-none name"><code>ML_Workflows.Rmd</code></div>
    </div>

    
    
<div class="sourceCode" id="cb1"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va">icardaFIGSr</span><span class="op">)</span></span>
<span><span class="co">#&gt; Error in get(paste0(generic, ".", class), envir = get_method_env()) : </span></span>
<span><span class="co">#&gt;   object 'type_sum.accel' not found</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://dplyr.tidyverse.org" class="external-link">dplyr</a></span><span class="op">)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Attaching package: 'dplyr'</span></span>
<span><span class="co">#&gt; The following objects are masked from 'package:stats':</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;     filter, lag</span></span>
<span><span class="co">#&gt; The following objects are masked from 'package:base':</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;     intersect, setdiff, setequal, union</span></span></code></pre></div>
<div class="section level2">
<h2 id="tunetrain">tuneTrain()<a class="anchor" aria-label="anchor" href="#tunetrain"></a>
</h2>
<p><code><a href="../reference/tuneTrain.html">tuneTrain()</a></code> retruns a list of dataframes, textoutputs
and plots. Each type represents key informations about the Machine
learning workflow process, such as, train/test tables, processing
transformations, fitted model and tuned model summaries, model
performance metrics, feature performance and predictions plots. Below we
explore three principle ML tasks that can be conducted using
<code><a href="../reference/tuneTrain.html">tuneTrain()</a></code> function.</p>
<div class="section level3">
<h3 id="binary-classification-balanced-data">Binary classification : Balanced data<a class="anchor" aria-label="anchor" href="#binary-classification-balanced-data"></a>
</h3>
<p><code>septoriaDurumWC</code> dataset is part of the icardaFIGSr
package and is designed for modeling climate impacts on the presence of
Septoria in durum wheat. It contains multiple columns representing
climate variables, which are used as predictors, and a response
variable, ST_S, which indicates the presence or severity of Septoria.
These two cases are labeled as <code>R</code> and <code>S</code> levels,
to describe <code>resistance</code> and <code>susceptibility</code>,
respectively.</p>
<p><br></p>
<p><strong>R (Resistant):</strong> Represents samples/observations that
are resistant to Septoria. This level indicates the wheat genotypes or
environmental conditions where Septoria’s impact is minimal or absent.
<br><strong>S (Susceptible):</strong> Represents samples/observations
that are susceptible to Septoria. This level indicates the wheat
genotypes or environmental conditions where Septoria’s impact is
significant. <br></p>
<p>These levels are critical for building and validating models to
classify resistance and susceptibility to Septoria under varying
climatic conditions.</p>
<div class="sourceCode" id="cb2"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/utils/data.html" class="external-link">data</a></span><span class="op">(</span><span class="st">"septoriaDurumWC"</span>, package <span class="op">=</span> <span class="st">"icardaFIGSr"</span><span class="op">)</span></span>
<span></span>
<span><span class="fu">dplyr</span><span class="fu">::</span><span class="fu"><a href="https://pillar.r-lib.org/reference/glimpse.html" class="external-link">glimpse</a></span><span class="op">(</span><span class="va">septoriaDurumWC</span><span class="op">)</span></span>
<span><span class="co">#&gt; Rows: 200</span></span>
<span><span class="co">#&gt; Columns: 56</span></span>
<span><span class="co">#&gt; $ ST_S   <span style="color: #949494; font-style: italic;">&lt;fct&gt;</span> S, R, R, R, R, S, S, R, R, S, R, R, R, S, R, S, S, R, R, S, R, …</span></span>
<span><span class="co">#&gt; $ tmin1  <span style="color: #949494; font-style: italic;">&lt;dbl&gt;</span> -52, -17, 53, -73, -123, 100, -304, -99, 28, 26, 6, 60, -44, -7…</span></span>
<span><span class="co">#&gt; $ tmin2  <span style="color: #949494; font-style: italic;">&lt;dbl&gt;</span> -40, -8, 66, -66, -94, 110, -268, -92, 37, 32, 21, 66, -29, -18…</span></span>
<span><span class="co">#&gt; $ tmin3  <span style="color: #949494; font-style: italic;">&lt;dbl&gt;</span> 4, 13, 87, -21, -65, 114, -161, -42, 58, 53, 36, 88, 1, 23, 93,…</span></span>
<span><span class="co">#&gt; $ tmin4  <span style="color: #949494; font-style: italic;">&lt;dbl&gt;</span> 58, 50, 120, 59, -16, 118, -35, 36, 86, 81, 76, 101, 42, 42, 12…</span></span>
<span><span class="co">#&gt; $ tmin5  <span style="color: #949494; font-style: italic;">&lt;dbl&gt;</span> 102, 86, 150, 118, 31, 116, 33, 97, 122, 117, 122, 118, 80, 91,…</span></span>
<span><span class="co">#&gt; $ tmin6  <span style="color: #949494; font-style: italic;">&lt;dbl&gt;</span> 146, 121, 198, 157, 73, 111, 101, 132, 167, 162, 176, 144, 111,…</span></span>
<span><span class="co">#&gt; $ tmin7  <span style="color: #949494; font-style: italic;">&lt;dbl&gt;</span> 187, 152, 235, 177, 100, 116, 144, 148, 207, 204, 227, 155, 137…</span></span>
<span><span class="co">#&gt; $ tmin8  <span style="color: #949494; font-style: italic;">&lt;dbl&gt;</span> 185, 148, 240, 167, 93, 107, 125, 138, 211, 207, 228, 157, 134,…</span></span>
<span><span class="co">#&gt; $ tmin9  <span style="color: #949494; font-style: italic;">&lt;dbl&gt;</span> 143, 112, 204, 116, 43, 101, 49, 88, 172, 169, 197, 148, 102, 1…</span></span>
<span><span class="co">#&gt; $ tmin10 <span style="color: #949494; font-style: italic;">&lt;dbl&gt;</span> 86, 69, 149, 55, 0, 91, -40, 31, 122, 116, 147, 125, 58, 80, 16…</span></span>
<span><span class="co">#&gt; $ tmin11 <span style="color: #949494; font-style: italic;">&lt;dbl&gt;</span> 26, 28, 96, 8, -64, 86, -163, -20, 71, 66, 102, 92, 17, 26, 103…</span></span>
<span><span class="co">#&gt; $ tmin12 <span style="color: #949494; font-style: italic;">&lt;dbl&gt;</span> -20, 0, 63, -37, -105, 84, -264, -63, 39, 34, 65, 67, -18, 1, 5…</span></span>
<span><span class="co">#&gt; $ tmax1  <span style="color: #949494; font-style: italic;">&lt;dbl&gt;</span> 14, 80, 121, -12, 5, 265, -170, -36, 107, 101, 122, 142, 26, 88…</span></span>
<span><span class="co">#&gt; $ tmax2  <span style="color: #949494; font-style: italic;">&lt;dbl&gt;</span> 33, 96, 133, -2, 36, 272, -114, -28, 122, 113, 139, 155, 53, 93…</span></span>
<span><span class="co">#&gt; $ tmax3  <span style="color: #949494; font-style: italic;">&lt;dbl&gt;</span> 88, 138, 158, 53, 66, 274, -17, 25, 147, 138, 166, 179, 105, 13…</span></span>
<span><span class="co">#&gt; $ tmax4  <span style="color: #949494; font-style: italic;">&lt;dbl&gt;</span> 154, 185, 177, 163, 127, 270, 96, 130, 176, 164, 208, 200, 158,…</span></span>
<span><span class="co">#&gt; $ tmax5  <span style="color: #949494; font-style: italic;">&lt;dbl&gt;</span> 213, 235, 230, 230, 180, 264, 180, 204, 220, 214, 273, 217, 207…</span></span>
<span><span class="co">#&gt; $ tmax6  <span style="color: #949494; font-style: italic;">&lt;dbl&gt;</span> 271, 282, 281, 269, 228, 242, 234, 239, 270, 266, 340, 257, 251…</span></span>
<span><span class="co">#&gt; $ tmax7  <span style="color: #949494; font-style: italic;">&lt;dbl&gt;</span> 320, 318, 328, 292, 278, 217, 255, 255, 320, 318, 384, 279, 288…</span></span>
<span><span class="co">#&gt; $ tmax8  <span style="color: #949494; font-style: italic;">&lt;dbl&gt;</span> 321, 319, 326, 286, 275, 227, 237, 249, 320, 317, 391, 287, 285…</span></span>
<span><span class="co">#&gt; $ tmax9  <span style="color: #949494; font-style: italic;">&lt;dbl&gt;</span> 278, 281, 284, 227, 211, 234, 177, 191, 275, 269, 371, 266, 246…</span></span>
<span><span class="co">#&gt; $ tmax10 <span style="color: #949494; font-style: italic;">&lt;dbl&gt;</span> 197, 215, 227, 144, 155, 253, 85, 115, 213, 206, 314, 226, 185,…</span></span>
<span><span class="co">#&gt; $ tmax11 <span style="color: #949494; font-style: italic;">&lt;dbl&gt;</span> 113, 147, 166, 68, 70, 254, -42, 35, 153, 146, 245, 174, 119, 1…</span></span>
<span><span class="co">#&gt; $ tmax12 <span style="color: #949494; font-style: italic;">&lt;dbl&gt;</span> 41, 93, 125, 20, 22, 258, -144, -9, 109, 102, 185, 144, 52, 90,…</span></span>
<span><span class="co">#&gt; $ prec1  <span style="color: #949494; font-style: italic;">&lt;dbl&gt;</span> 64, 97, 64, 50, 17, 14, 5, 45, 52, 59, 57, 130, 47, 29, 13, 38,…</span></span>
<span><span class="co">#&gt; $ prec2  <span style="color: #949494; font-style: italic;">&lt;dbl&gt;</span> 62, 74, 51, 38, 11, 33, 5, 34, 48, 48, 52, 125, 37, 30, 10, 57,…</span></span>
<span><span class="co">#&gt; $ prec3  <span style="color: #949494; font-style: italic;">&lt;dbl&gt;</span> 73, 57, 46, 37, 18, 49, 9, 34, 46, 48, 48, 77, 37, 36, 15, 80, …</span></span>
<span><span class="co">#&gt; $ prec4  <span style="color: #949494; font-style: italic;">&lt;dbl&gt;</span> 81, 40, 40, 46, 28, 75, 26, 40, 39, 36, 14, 83, 47, 31, 10, 75,…</span></span>
<span><span class="co">#&gt; $ prec5  <span style="color: #949494; font-style: italic;">&lt;dbl&gt;</span> 63, 39, 39, 52, 74, 79, 52, 49, 36, 43, 4, 71, 50, 28, 7, 24, 3…</span></span>
<span><span class="co">#&gt; $ prec6  <span style="color: #949494; font-style: italic;">&lt;dbl&gt;</span> 18, 21, 11, 62, 73, 149, 100, 65, 16, 18, 0, 41, 32, 27, 2, 1, …</span></span>
<span><span class="co">#&gt; $ prec7  <span style="color: #949494; font-style: italic;">&lt;dbl&gt;</span> 4, 14, 1, 58, 43, 222, 161, 68, 3, 3, 0, 10, 12, 10, 1, 6, 11, …</span></span>
<span><span class="co">#&gt; $ prec8  <span style="color: #949494; font-style: italic;">&lt;dbl&gt;</span> 2, 11, 2, 44, 42, 220, 144, 51, 7, 4, 1, 12, 7, 12, 2, 2, 17, 3…</span></span>
<span><span class="co">#&gt; $ prec9  <span style="color: #949494; font-style: italic;">&lt;dbl&gt;</span> 7, 15, 20, 43, 34, 127, 81, 48, 20, 22, 0, 46, 15, 27, 9, 2, 34…</span></span>
<span><span class="co">#&gt; $ prec10 <span style="color: #949494; font-style: italic;">&lt;dbl&gt;</span> 50, 38, 41, 32, 22, 36, 29, 43, 34, 37, 6, 98, 27, 39, 13, 7, 3…</span></span>
<span><span class="co">#&gt; $ prec11 <span style="color: #949494; font-style: italic;">&lt;dbl&gt;</span> 66, 60, 63, 47, 14, 15, 11, 46, 51, 55, 29, 123, 30, 29, 14, 16…</span></span>
<span><span class="co">#&gt; $ prec12 <span style="color: #949494; font-style: italic;">&lt;dbl&gt;</span> 72, 99, 62, 66, 14, 8, 7, 47, 53, 57, 59, 118, 45, 31, 13, 28, …</span></span>
<span><span class="co">#&gt; $ bio1   <span style="color: #949494; font-style: italic;">&lt;dbl&gt;</span> 119, 131, 175, 100, 63, 178, 0, 72, 156, 151, 189, 160, 107, 13…</span></span>
<span><span class="co">#&gt; $ bio2   <span style="color: #949494; font-style: italic;">&lt;dbl&gt;</span> 102, 136, 75, 90, 148, 148, 130, 85, 93, 91, 145, 100, 115, 123…</span></span>
<span><span class="co">#&gt; $ bio3   <span style="color: #949494; font-style: italic;">&lt;dbl&gt;</span> 27, 40, 27, 24, 36, 77, 23, 24, 31, 31, 37, 44, 34, 36, 37, 36,…</span></span>
<span><span class="co">#&gt; $ bio4   <span style="color: #949494; font-style: italic;">&lt;dbl&gt;</span> 9503, 7151, 6938, 9934, 8461, 1051, 15232, 9756, 6945, 7074, 84…</span></span>
<span><span class="co">#&gt; $ bio5   <span style="color: #949494; font-style: italic;">&lt;dbl&gt;</span> 321, 319, 328, 292, 278, 274, 255, 255, 320, 318, 391, 287, 288…</span></span>
<span><span class="co">#&gt; $ bio6   <span style="color: #949494; font-style: italic;">&lt;dbl&gt;</span> -52, -17, 53, -73, -123, 84, -304, -99, 28, 26, 6, 60, -44, -18…</span></span>
<span><span class="co">#&gt; $ bio7   <span style="color: #949494; font-style: italic;">&lt;dbl&gt;</span> 373, 336, 275, 365, 401, 190, 559, 354, 292, 292, 385, 227, 332…</span></span>
<span><span class="co">#&gt; $ bio8   <span style="color: #949494; font-style: italic;">&lt;dbl&gt;</span> 103, 40, 103, 207, 147, 169, 182, 193, 84, 79, 89, 105, 98, 87,…</span></span>
<span><span class="co">#&gt; $ bio9   <span style="color: #949494; font-style: italic;">&lt;dbl&gt;</span> 238, 221, 267, 165, -44, 174, -211, 4, 248, 245, 290, 212, 198,…</span></span>
<span><span class="co">#&gt; $ bio10  <span style="color: #949494; font-style: italic;">&lt;dbl&gt;</span> 238, 223, 269, 224, 174, 192, 182, 193, 250, 247, 299, 215, 200…</span></span>
<span><span class="co">#&gt; $ bio11  <span style="color: #949494; font-style: italic;">&lt;dbl&gt;</span> -5, 40, 93, -29, -44, 167, -211, -55, 73, 67, 81, 105, 6, 40, 1…</span></span>
<span><span class="co">#&gt; $ bio12  <span style="color: #949494; font-style: italic;">&lt;dbl&gt;</span> 562, 565, 440, 575, 390, 1027, 630, 570, 405, 430, 270, 934, 38…</span></span>
<span><span class="co">#&gt; $ bio13  <span style="color: #949494; font-style: italic;">&lt;dbl&gt;</span> 81, 99, 64, 66, 74, 222, 161, 68, 53, 59, 59, 130, 50, 39, 15, …</span></span>
<span><span class="co">#&gt; $ bio14  <span style="color: #949494; font-style: italic;">&lt;dbl&gt;</span> 2, 11, 1, 32, 11, 8, 5, 34, 3, 3, 0, 10, 7, 10, 1, 1, 11, 3, 16…</span></span>
<span><span class="co">#&gt; $ bio15  <span style="color: #949494; font-style: italic;">&lt;dbl&gt;</span> 61, 62, 60, 21, 64, 86, 101, 21, 51, 53, 103, 53, 42, 29, 48, 9…</span></span>
<span><span class="co">#&gt; $ bio16  <span style="color: #949494; font-style: italic;">&lt;dbl&gt;</span> 217, 270, 189, 172, 190, 591, 405, 184, 156, 171, 168, 373, 134…</span></span>
<span><span class="co">#&gt; $ bio17  <span style="color: #949494; font-style: italic;">&lt;dbl&gt;</span> 13, 40, 14, 119, 42, 37, 17, 108, 26, 25, 1, 63, 34, 49, 5, 9, …</span></span>
<span><span class="co">#&gt; $ bio18  <span style="color: #949494; font-style: italic;">&lt;dbl&gt;</span> 13, 46, 23, 164, 158, 157, 405, 184, 30, 29, 1, 68, 51, 49, 5, …</span></span>
<span><span class="co">#&gt; $ bio19  <span style="color: #949494; font-style: italic;">&lt;dbl&gt;</span> 198, 270, 177, 154, 42, 569, 17, 126, 153, 164, 157, 373, 129, …</span></span></code></pre></div>
<div class="sourceCode" id="cb3"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span></span>
<span><span class="co"># Lets select some random predictors for simplicity. </span></span>
<span><span class="va">random_cols</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/sample.html" class="external-link">sample</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/colnames.html" class="external-link">colnames</a></span><span class="op">(</span><span class="va">septoriaDurumWC</span><span class="op">)</span>, <span class="fl">7</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Note : we can still use the whole dataset for model training and testing.</span></span>
<span></span>
<span><span class="co"># Select Y and predictors.</span></span>
<span><span class="va">septoriaDurumWC_sample</span> <span class="op">&lt;-</span> <span class="va">septoriaDurumWC</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html" class="external-link">%&gt;%</a></span></span>
<span>   <span class="fu">dplyr</span><span class="fu">::</span><span class="fu"><a href="https://dplyr.tidyverse.org/reference/select.html" class="external-link">select</a></span><span class="op">(</span><span class="va">ST_S</span>, <span class="fu"><a href="https://tidyselect.r-lib.org/reference/all_of.html" class="external-link">all_of</a></span><span class="op">(</span><span class="va">random_cols</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># check results</span></span>
<span><span class="va">septoriaDurumWC_sample</span></span>
<span><span class="co">#&gt; <span style="color: #949494;"># A tibble: 200 × 8</span></span></span>
<span><span class="co">#&gt;    ST_S   bio8 tmax10 tmin11 prec12  bio1 bio10 bio14</span></span>
<span><span class="co">#&gt;    <span style="color: #949494; font-style: italic;">&lt;fct&gt;</span> <span style="color: #949494; font-style: italic;">&lt;dbl&gt;</span>  <span style="color: #949494; font-style: italic;">&lt;dbl&gt;</span>  <span style="color: #949494; font-style: italic;">&lt;dbl&gt;</span>  <span style="color: #949494; font-style: italic;">&lt;dbl&gt;</span> <span style="color: #949494; font-style: italic;">&lt;dbl&gt;</span> <span style="color: #949494; font-style: italic;">&lt;dbl&gt;</span> <span style="color: #949494; font-style: italic;">&lt;dbl&gt;</span></span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;"> 1</span> S       103    197     26     72   119   238     2</span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;"> 2</span> R        40    215     28     99   131   223    11</span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;"> 3</span> R       103    227     96     62   175   269     1</span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;"> 4</span> R       207    144      8     66   100   224    32</span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;"> 5</span> R       147    155    -<span style="color: #BB0000;">64</span>     14    63   174    11</span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;"> 6</span> S       169    253     86      8   178   192     8</span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;"> 7</span> S       182     85   -<span style="color: #BB0000;">163</span>      7     0   182     5</span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;"> 8</span> R       193    115    -<span style="color: #BB0000;">20</span>     47    72   193    34</span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;"> 9</span> R        84    213     71     53   156   250     3</span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;">10</span> S        79    206     66     57   151   247     3</span></span>
<span><span class="co">#&gt; <span style="color: #949494;"># ℹ 190 more rows</span></span></span></code></pre></div>
<div class="sourceCode" id="cb4"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span></span>
<span><span class="co"># Count classes (balance check) </span></span>
<span><span class="va">septoriaDurumWC_sample</span><span class="op">|&gt;</span></span>
<span>   <span class="fu"><a href="https://dplyr.tidyverse.org/reference/count.html" class="external-link">count</a></span><span class="op">(</span><span class="va">ST_S</span><span class="op">)</span></span>
<span><span class="co">#&gt; <span style="color: #949494;"># A tibble: 2 × 2</span></span></span>
<span><span class="co">#&gt;   ST_S      n</span></span>
<span><span class="co">#&gt;   <span style="color: #949494; font-style: italic;">&lt;fct&gt;</span> <span style="color: #949494; font-style: italic;">&lt;int&gt;</span></span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;">1</span> R       106</span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;">2</span> S        94</span></span></code></pre></div>
<div class="sourceCode" id="cb5"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://xrobin.github.io/pROC/" class="external-link">pROC</a></span><span class="op">)</span></span>
<span><span class="co">#&gt; Type 'citation("pROC")' for a citation.</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Attaching package: 'pROC'</span></span>
<span><span class="co">#&gt; The following objects are masked from 'package:stats':</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;     cov, smooth, var</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://github.com/topepo/caret/" class="external-link">caret</a></span><span class="op">)</span></span>
<span><span class="co">#&gt; Loading required package: ggplot2</span></span>
<span><span class="co">#&gt; Loading required package: lattice</span></span>
<span></span>
<span><span class="co">## Run Binary classification of ST_S with balanced data </span></span>
<span></span>
<span><span class="va">knn.ST_S</span> <span class="op">&lt;-</span> <span class="fu">icardaFIGSr</span><span class="fu">::</span><span class="fu"><a href="../reference/tuneTrain.html">tuneTrain</a></span><span class="op">(</span></span>
<span>                      data <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/as.data.frame.html" class="external-link">as.data.frame</a></span><span class="op">(</span><span class="va">septoriaDurumWC_sample</span><span class="op">)</span>, </span>
<span>                      y <span class="op">=</span>  <span class="st">'ST_S'</span>, </span>
<span>                      method <span class="op">=</span> <span class="st">'knn'</span>, <span class="co"># using knn algorithm </span></span>
<span>                      summary <span class="op">=</span> <span class="va">multiClassSummary</span>, <span class="co"># Important for classification tasks </span></span>
<span>                      repeats <span class="op">=</span> <span class="fl">3</span>,</span>
<span>                      classProbs <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span> <span class="co"># also important for classification tasks </span></span>
<span><span class="co">#&gt; k-Nearest Neighbors </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; 141 samples</span></span>
<span><span class="co">#&gt;   7 predictor</span></span>
<span><span class="co">#&gt;   2 classes: 'R', 'S' </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Pre-processing: centered (7), scaled (7) </span></span>
<span><span class="co">#&gt; Resampling: Cross-Validated (10 fold, repeated 3 times) </span></span>
<span><span class="co">#&gt; Summary of sample sizes: 127, 128, 128, 127, 126, 126, ... </span></span>
<span><span class="co">#&gt; Resampling results across tuning parameters:</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;   k   logLoss    AUC        prAUC      Accuracy   Kappa      F1       </span></span>
<span><span class="co">#&gt;    5  2.4942735  0.6668863  0.4970950  0.6467399  0.2966753  0.6314547</span></span>
<span><span class="co">#&gt;    7  1.2124431  0.6569870  0.5001362  0.6476923  0.3012302  0.6196532</span></span>
<span><span class="co">#&gt;    9  1.1444721  0.6486253  0.5238724  0.6380342  0.2809037  0.6147496</span></span>
<span><span class="co">#&gt;   11  1.0744261  0.6490363  0.5129636  0.6142002  0.2330827  0.5925331</span></span>
<span><span class="co">#&gt;   13  0.9313736  0.6114654  0.5041211  0.5984860  0.2042462  0.5744044</span></span>
<span><span class="co">#&gt;   15  0.9258012  0.6094459  0.5061685  0.5887302  0.1840823  0.5627878</span></span>
<span><span class="co">#&gt;   17  0.9262011  0.6070649  0.5165494  0.5921368  0.1923800  0.5646618</span></span>
<span><span class="co">#&gt;   19  0.7683714  0.6105159  0.5086565  0.5821123  0.1728507  0.5438374</span></span>
<span><span class="co">#&gt;   21  0.6807348  0.6178571  0.5249527  0.5982540  0.2082104  0.5483432</span></span>
<span><span class="co">#&gt;   23  0.6816755  0.6154691  0.5014774  0.5917705  0.1943961  0.5480329</span></span>
<span><span class="co">#&gt;   Sensitivity  Specificity  Pos_Pred_Value  Neg_Pred_Value  Precision</span></span>
<span><span class="co">#&gt;   0.5880952    0.7095238    0.7081746       0.6053319       0.7081746</span></span>
<span><span class="co">#&gt;   0.5654762    0.7380952    0.7208069       0.6069372       0.7208069</span></span>
<span><span class="co">#&gt;   0.5654762    0.7190476    0.7070635       0.5949471       0.7070635</span></span>
<span><span class="co">#&gt;   0.5470238    0.6880952    0.6789021       0.5768074       0.6789021</span></span>
<span><span class="co">#&gt;   0.5309524    0.6753968    0.6682011       0.5651852       0.6682011</span></span>
<span><span class="co">#&gt;   0.5172619    0.6682540    0.6603800       0.5567400       0.6603800</span></span>
<span><span class="co">#&gt;   0.5142857    0.6801587    0.6647090       0.5573906       0.6647090</span></span>
<span><span class="co">#&gt;   0.4946429    0.6793651    0.6433598       0.5489153       0.6433598</span></span>
<span><span class="co">#&gt;   0.4767857    0.7341270    0.6930556       0.5566450       0.6930556</span></span>
<span><span class="co">#&gt;   0.4815476    0.7150794    0.6715212       0.5529810       0.6715212</span></span>
<span><span class="co">#&gt;   Recall     Detection_Rate  Balanced_Accuracy</span></span>
<span><span class="co">#&gt;   0.5880952  0.3119658       0.6488095        </span></span>
<span><span class="co">#&gt;   0.5654762  0.3005128       0.6517857        </span></span>
<span><span class="co">#&gt;   0.5654762  0.3001954       0.6422619        </span></span>
<span><span class="co">#&gt;   0.5470238  0.2908303       0.6175595        </span></span>
<span><span class="co">#&gt;   0.5309524  0.2817827       0.6031746        </span></span>
<span><span class="co">#&gt;   0.5172619  0.2742979       0.5927579        </span></span>
<span><span class="co">#&gt;   0.5142857  0.2727350       0.5972222        </span></span>
<span><span class="co">#&gt;   0.4946429  0.2630525       0.5870040        </span></span>
<span><span class="co">#&gt;   0.4767857  0.2534799       0.6054563        </span></span>
<span><span class="co">#&gt;   0.4815476  0.2560195       0.5983135        </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Accuracy was used to select the optimal model using the largest value.</span></span>
<span><span class="co">#&gt; The final value used for the model was k = 7.</span></span>
<span><span class="co">#&gt; k-Nearest Neighbors </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; 141 samples</span></span>
<span><span class="co">#&gt;   7 predictor</span></span>
<span><span class="co">#&gt;   2 classes: 'R', 'S' </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Pre-processing: centered (7), scaled (7) </span></span>
<span><span class="co">#&gt; Resampling: Cross-Validated (10 fold, repeated 3 times) </span></span>
<span><span class="co">#&gt; Summary of sample sizes: 127, 127, 127, 128, 127, 126, ... </span></span>
<span><span class="co">#&gt; Resampling results across tuning parameters:</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;   k  logLoss   AUC        prAUC      Accuracy   Kappa      F1       </span></span>
<span><span class="co">#&gt;   5  2.419047  0.6766440  0.4987306  0.6488156  0.3023780  0.6329208</span></span>
<span><span class="co">#&gt;   6  1.650332  0.6698625  0.4961783  0.6473016  0.2981138  0.6306002</span></span>
<span><span class="co">#&gt;   7  1.352115  0.6622166  0.4871674  0.6575214  0.3193743  0.6404283</span></span>
<span><span class="co">#&gt;   8  1.124306  0.6655612  0.5046772  0.6295360  0.2659140  0.6051478</span></span>
<span><span class="co">#&gt;   9  1.117565  0.6627480  0.5225668  0.6455433  0.2947085  0.6287166</span></span>
<span><span class="co">#&gt;   Sensitivity  Specificity  Pos_Pred_Value  Neg_Pred_Value  Precision</span></span>
<span><span class="co">#&gt;   0.5791667    0.7277778    0.7264683       0.6024747       0.7264683</span></span>
<span><span class="co">#&gt;   0.5797619    0.7230159    0.7285979       0.6033634       0.7285979</span></span>
<span><span class="co">#&gt;   0.5869048    0.7373016    0.7375998       0.6127345       0.7375998</span></span>
<span><span class="co">#&gt;   0.5535714    0.7166667    0.6993723       0.5912049       0.6993723</span></span>
<span><span class="co">#&gt;   0.5744048    0.7253968    0.7315873       0.5992027       0.7315873</span></span>
<span><span class="co">#&gt;   Recall     Detection_Rate  Balanced_Accuracy</span></span>
<span><span class="co">#&gt;   0.5791667  0.3083761       0.6534722        </span></span>
<span><span class="co">#&gt;   0.5797619  0.3090842       0.6513889        </span></span>
<span><span class="co">#&gt;   0.5869048  0.3128205       0.6621032        </span></span>
<span><span class="co">#&gt;   0.5535714  0.2940415       0.6351190        </span></span>
<span><span class="co">#&gt;   0.5744048  0.3058364       0.6499008        </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Accuracy was used to select the optimal model using the largest value.</span></span>
<span><span class="co">#&gt; The final value used for the model was k = 7.</span></span>
<span><span class="co">#&gt; Setting levels: control = R, case = S</span></span>
<span><span class="co">#&gt; Setting direction: controls &lt; cases</span></span></code></pre></div>
<p>let’s explore the results.</p>
<div class="sourceCode" id="cb6"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Class probabilities table</span></span>
<span><span class="va">knn.ST_S</span><span class="op">$</span><span class="va">`Class Probabilities`</span></span>
<span><span class="co">#&gt;            R         S</span></span>
<span><span class="co">#&gt; 1  0.4285714 0.5714286</span></span>
<span><span class="co">#&gt; 2  0.8571429 0.1428571</span></span>
<span><span class="co">#&gt; 3  0.2857143 0.7142857</span></span>
<span><span class="co">#&gt; 4  0.4285714 0.5714286</span></span>
<span><span class="co">#&gt; 5  0.2857143 0.7142857</span></span>
<span><span class="co">#&gt; 6  0.8571429 0.1428571</span></span>
<span><span class="co">#&gt; 7  0.2857143 0.7142857</span></span>
<span><span class="co">#&gt; 8  0.2857143 0.7142857</span></span>
<span><span class="co">#&gt; 9  0.2857143 0.7142857</span></span>
<span><span class="co">#&gt; 10 0.5714286 0.4285714</span></span>
<span><span class="co">#&gt; 11 1.0000000 0.0000000</span></span>
<span><span class="co">#&gt; 12 0.4285714 0.5714286</span></span>
<span><span class="co">#&gt; 13 0.4285714 0.5714286</span></span>
<span><span class="co">#&gt; 14 0.8571429 0.1428571</span></span>
<span><span class="co">#&gt; 15 0.1428571 0.8571429</span></span>
<span><span class="co">#&gt; 16 0.2857143 0.7142857</span></span>
<span><span class="co">#&gt; 17 1.0000000 0.0000000</span></span>
<span><span class="co">#&gt; 18 0.7142857 0.2857143</span></span>
<span><span class="co">#&gt; 19 0.8571429 0.1428571</span></span>
<span><span class="co">#&gt; 20 0.5714286 0.4285714</span></span>
<span><span class="co">#&gt; 21 0.7142857 0.2857143</span></span>
<span><span class="co">#&gt; 22 0.5714286 0.4285714</span></span>
<span><span class="co">#&gt; 23 0.2857143 0.7142857</span></span>
<span><span class="co">#&gt; 24 0.5714286 0.4285714</span></span>
<span><span class="co">#&gt; 25 0.2857143 0.7142857</span></span>
<span><span class="co">#&gt; 26 1.0000000 0.0000000</span></span>
<span><span class="co">#&gt; 27 0.2857143 0.7142857</span></span>
<span><span class="co">#&gt; 28 0.4285714 0.5714286</span></span>
<span><span class="co">#&gt; 29 0.4285714 0.5714286</span></span>
<span><span class="co">#&gt; 30 0.4285714 0.5714286</span></span>
<span><span class="co">#&gt; 31 0.2857143 0.7142857</span></span>
<span><span class="co">#&gt; 32 0.4285714 0.5714286</span></span>
<span><span class="co">#&gt; 33 1.0000000 0.0000000</span></span>
<span><span class="co">#&gt; 34 0.2857143 0.7142857</span></span>
<span><span class="co">#&gt; 35 0.2857143 0.7142857</span></span>
<span><span class="co">#&gt; 36 0.5714286 0.4285714</span></span>
<span><span class="co">#&gt; 37 0.7142857 0.2857143</span></span>
<span><span class="co">#&gt; 38 0.7142857 0.2857143</span></span>
<span><span class="co">#&gt; 39 0.8571429 0.1428571</span></span>
<span><span class="co">#&gt; 40 0.5714286 0.4285714</span></span>
<span><span class="co">#&gt; 41 0.2857143 0.7142857</span></span>
<span><span class="co">#&gt; 42 0.2857143 0.7142857</span></span>
<span><span class="co">#&gt; 43 1.0000000 0.0000000</span></span>
<span><span class="co">#&gt; 44 0.8750000 0.1250000</span></span>
<span><span class="co">#&gt; 45 0.4285714 0.5714286</span></span>
<span><span class="co">#&gt; 46 0.2857143 0.7142857</span></span>
<span><span class="co">#&gt; 47 0.5714286 0.4285714</span></span>
<span><span class="co">#&gt; 48 0.1428571 0.8571429</span></span>
<span><span class="co">#&gt; 49 0.8571429 0.1428571</span></span>
<span><span class="co">#&gt; 50 0.8571429 0.1428571</span></span>
<span><span class="co">#&gt; 51 0.4285714 0.5714286</span></span>
<span><span class="co">#&gt; 52 0.4285714 0.5714286</span></span>
<span><span class="co">#&gt; 53 0.5714286 0.4285714</span></span>
<span><span class="co">#&gt; 54 1.0000000 0.0000000</span></span>
<span><span class="co">#&gt; 55 0.5714286 0.4285714</span></span>
<span><span class="co">#&gt; 56 0.8571429 0.1428571</span></span>
<span><span class="co">#&gt; 57 0.8571429 0.1428571</span></span>
<span><span class="co">#&gt; 58 0.4285714 0.5714286</span></span>
<span><span class="co">#&gt; 59 0.4285714 0.5714286</span></span></code></pre></div>
<div class="sourceCode" id="cb7"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co">## Plot class probabilities</span></span>
<span><span class="va">knn.ST_S</span><span class="op">$</span><span class="va">`Class Probabilities Plot`</span></span></code></pre></div>
<div class="figure">
<img src="ML_Workflows_files/figure-html/unnamed-chunk-7-1.png" alt="Class probabilities ST_S knn" width="672"><p class="caption">
Class probabilities ST_S knn
</p>
</div>
<div class="sourceCode" id="cb8"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># ROC plot</span></span>
<span><span class="va">knn.ST_S</span><span class="op">$</span><span class="va">ROC_Plot</span></span></code></pre></div>
<div class="figure">
<img src="ML_Workflows_files/figure-html/unnamed-chunk-8-1.png" alt="Roc plot ST_S knn" width="672"><p class="caption">
Roc plot ST_S knn
</p>
</div>
<div class="sourceCode" id="cb9"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Variable importance</span></span>
<span><span class="va">knn.ST_S</span><span class="op">$</span><span class="va">Variableimportance</span></span></code></pre></div>
<div class="figure">
<img src="ML_Workflows_files/figure-html/unnamed-chunk-9-1.png" alt="Variable Importance ST_S knn" width="672"><p class="caption">
Variable Importance ST_S knn
</p>
</div>
<div class="sourceCode" id="cb10"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Model quality</span></span>
<span><span class="va">knn.ST_S</span><span class="op">$</span><span class="va">`Model quality`</span></span>
<span><span class="co">#&gt; Confusion Matrix and Statistics</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;           Reference</span></span>
<span><span class="co">#&gt; Prediction  R  S</span></span>
<span><span class="co">#&gt;          R 22  7</span></span>
<span><span class="co">#&gt;          S  9 21</span></span>
<span><span class="co">#&gt;                                           </span></span>
<span><span class="co">#&gt;                Accuracy : 0.7288          </span></span>
<span><span class="co">#&gt;                  95% CI : (0.5973, 0.8364)</span></span>
<span><span class="co">#&gt;     No Information Rate : 0.5254          </span></span>
<span><span class="co">#&gt;     P-Value [Acc &gt; NIR] : 0.00113         </span></span>
<span><span class="co">#&gt;                                           </span></span>
<span><span class="co">#&gt;                   Kappa : 0.4581          </span></span>
<span><span class="co">#&gt;                                           </span></span>
<span><span class="co">#&gt;  Mcnemar's Test P-Value : 0.80259         </span></span>
<span><span class="co">#&gt;                                           </span></span>
<span><span class="co">#&gt;             Sensitivity : 0.7097          </span></span>
<span><span class="co">#&gt;             Specificity : 0.7500          </span></span>
<span><span class="co">#&gt;          Pos Pred Value : 0.7586          </span></span>
<span><span class="co">#&gt;          Neg Pred Value : 0.7000          </span></span>
<span><span class="co">#&gt;              Prevalence : 0.5254          </span></span>
<span><span class="co">#&gt;          Detection Rate : 0.3729          </span></span>
<span><span class="co">#&gt;    Detection Prevalence : 0.4915          </span></span>
<span><span class="co">#&gt;       Balanced Accuracy : 0.7298          </span></span>
<span><span class="co">#&gt;                                           </span></span>
<span><span class="co">#&gt;        'Positive' Class : R               </span></span>
<span><span class="co">#&gt; </span></span></code></pre></div>
<div class="sourceCode" id="cb11"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Training</span></span>
<span><span class="va">knn.ST_S</span><span class="op">$</span><span class="va">Training</span></span>
<span><span class="co">#&gt; k-Nearest Neighbors </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; 141 samples</span></span>
<span><span class="co">#&gt;   7 predictor</span></span>
<span><span class="co">#&gt;   2 classes: 'R', 'S' </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Pre-processing: centered (7), scaled (7) </span></span>
<span><span class="co">#&gt; Resampling: Cross-Validated (10 fold, repeated 3 times) </span></span>
<span><span class="co">#&gt; Summary of sample sizes: 127, 127, 127, 128, 127, 126, ... </span></span>
<span><span class="co">#&gt; Resampling results across tuning parameters:</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;   k  logLoss   AUC        prAUC      Accuracy   Kappa      F1       </span></span>
<span><span class="co">#&gt;   5  2.419047  0.6766440  0.4987306  0.6488156  0.3023780  0.6329208</span></span>
<span><span class="co">#&gt;   6  1.650332  0.6698625  0.4961783  0.6473016  0.2981138  0.6306002</span></span>
<span><span class="co">#&gt;   7  1.352115  0.6622166  0.4871674  0.6575214  0.3193743  0.6404283</span></span>
<span><span class="co">#&gt;   8  1.124306  0.6655612  0.5046772  0.6295360  0.2659140  0.6051478</span></span>
<span><span class="co">#&gt;   9  1.117565  0.6627480  0.5225668  0.6455433  0.2947085  0.6287166</span></span>
<span><span class="co">#&gt;   Sensitivity  Specificity  Pos_Pred_Value  Neg_Pred_Value  Precision</span></span>
<span><span class="co">#&gt;   0.5791667    0.7277778    0.7264683       0.6024747       0.7264683</span></span>
<span><span class="co">#&gt;   0.5797619    0.7230159    0.7285979       0.6033634       0.7285979</span></span>
<span><span class="co">#&gt;   0.5869048    0.7373016    0.7375998       0.6127345       0.7375998</span></span>
<span><span class="co">#&gt;   0.5535714    0.7166667    0.6993723       0.5912049       0.6993723</span></span>
<span><span class="co">#&gt;   0.5744048    0.7253968    0.7315873       0.5992027       0.7315873</span></span>
<span><span class="co">#&gt;   Recall     Detection_Rate  Balanced_Accuracy</span></span>
<span><span class="co">#&gt;   0.5791667  0.3083761       0.6534722        </span></span>
<span><span class="co">#&gt;   0.5797619  0.3090842       0.6513889        </span></span>
<span><span class="co">#&gt;   0.5869048  0.3128205       0.6621032        </span></span>
<span><span class="co">#&gt;   0.5535714  0.2940415       0.6351190        </span></span>
<span><span class="co">#&gt;   0.5744048  0.3058364       0.6499008        </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Accuracy was used to select the optimal model using the largest value.</span></span>
<span><span class="co">#&gt; The final value used for the model was k = 7.</span></span></code></pre></div>
<div class="sourceCode" id="cb12"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Tuning</span></span>
<span><span class="va">knn.ST_S</span><span class="op">$</span><span class="va">Tuning</span></span>
<span><span class="co">#&gt; k-Nearest Neighbors </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; 141 samples</span></span>
<span><span class="co">#&gt;   7 predictor</span></span>
<span><span class="co">#&gt;   2 classes: 'R', 'S' </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Pre-processing: centered (7), scaled (7) </span></span>
<span><span class="co">#&gt; Resampling: Cross-Validated (10 fold, repeated 3 times) </span></span>
<span><span class="co">#&gt; Summary of sample sizes: 127, 128, 128, 127, 126, 126, ... </span></span>
<span><span class="co">#&gt; Resampling results across tuning parameters:</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;   k   logLoss    AUC        prAUC      Accuracy   Kappa      F1       </span></span>
<span><span class="co">#&gt;    5  2.4942735  0.6668863  0.4970950  0.6467399  0.2966753  0.6314547</span></span>
<span><span class="co">#&gt;    7  1.2124431  0.6569870  0.5001362  0.6476923  0.3012302  0.6196532</span></span>
<span><span class="co">#&gt;    9  1.1444721  0.6486253  0.5238724  0.6380342  0.2809037  0.6147496</span></span>
<span><span class="co">#&gt;   11  1.0744261  0.6490363  0.5129636  0.6142002  0.2330827  0.5925331</span></span>
<span><span class="co">#&gt;   13  0.9313736  0.6114654  0.5041211  0.5984860  0.2042462  0.5744044</span></span>
<span><span class="co">#&gt;   15  0.9258012  0.6094459  0.5061685  0.5887302  0.1840823  0.5627878</span></span>
<span><span class="co">#&gt;   17  0.9262011  0.6070649  0.5165494  0.5921368  0.1923800  0.5646618</span></span>
<span><span class="co">#&gt;   19  0.7683714  0.6105159  0.5086565  0.5821123  0.1728507  0.5438374</span></span>
<span><span class="co">#&gt;   21  0.6807348  0.6178571  0.5249527  0.5982540  0.2082104  0.5483432</span></span>
<span><span class="co">#&gt;   23  0.6816755  0.6154691  0.5014774  0.5917705  0.1943961  0.5480329</span></span>
<span><span class="co">#&gt;   Sensitivity  Specificity  Pos_Pred_Value  Neg_Pred_Value  Precision</span></span>
<span><span class="co">#&gt;   0.5880952    0.7095238    0.7081746       0.6053319       0.7081746</span></span>
<span><span class="co">#&gt;   0.5654762    0.7380952    0.7208069       0.6069372       0.7208069</span></span>
<span><span class="co">#&gt;   0.5654762    0.7190476    0.7070635       0.5949471       0.7070635</span></span>
<span><span class="co">#&gt;   0.5470238    0.6880952    0.6789021       0.5768074       0.6789021</span></span>
<span><span class="co">#&gt;   0.5309524    0.6753968    0.6682011       0.5651852       0.6682011</span></span>
<span><span class="co">#&gt;   0.5172619    0.6682540    0.6603800       0.5567400       0.6603800</span></span>
<span><span class="co">#&gt;   0.5142857    0.6801587    0.6647090       0.5573906       0.6647090</span></span>
<span><span class="co">#&gt;   0.4946429    0.6793651    0.6433598       0.5489153       0.6433598</span></span>
<span><span class="co">#&gt;   0.4767857    0.7341270    0.6930556       0.5566450       0.6930556</span></span>
<span><span class="co">#&gt;   0.4815476    0.7150794    0.6715212       0.5529810       0.6715212</span></span>
<span><span class="co">#&gt;   Recall     Detection_Rate  Balanced_Accuracy</span></span>
<span><span class="co">#&gt;   0.5880952  0.3119658       0.6488095        </span></span>
<span><span class="co">#&gt;   0.5654762  0.3005128       0.6517857        </span></span>
<span><span class="co">#&gt;   0.5654762  0.3001954       0.6422619        </span></span>
<span><span class="co">#&gt;   0.5470238  0.2908303       0.6175595        </span></span>
<span><span class="co">#&gt;   0.5309524  0.2817827       0.6031746        </span></span>
<span><span class="co">#&gt;   0.5172619  0.2742979       0.5927579        </span></span>
<span><span class="co">#&gt;   0.5142857  0.2727350       0.5972222        </span></span>
<span><span class="co">#&gt;   0.4946429  0.2630525       0.5870040        </span></span>
<span><span class="co">#&gt;   0.4767857  0.2534799       0.6054563        </span></span>
<span><span class="co">#&gt;   0.4815476  0.2560195       0.5983135        </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Accuracy was used to select the optimal model using the largest value.</span></span>
<span><span class="co">#&gt; The final value used for the model was k = 7.</span></span></code></pre></div>
<p><strong>Note the difference between Model Training and Tuning objects
at the end of the output. The final hyperparameter value is different (3
vs 5). </strong></p>
<div class="sourceCode" id="cb13"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># We also have access to the training and testing datasets in results list following the same logic</span></span>
<span></span>
<span><span class="va">knn.ST_S</span><span class="op">$</span><span class="va">`Training Data`</span></span>
<span><span class="co">#&gt;     ST_S bio8 tmax10 tmin11 prec12 bio1 bio10 bio14</span></span>
<span><span class="co">#&gt; 3      R  103    227     96     62  175   269     1</span></span>
<span><span class="co">#&gt; 4      R  207    144      8     66  100   224    32</span></span>
<span><span class="co">#&gt; 5      R  147    155    -64     14   63   174    11</span></span>
<span><span class="co">#&gt; 6      S  169    253     86      8  178   192     8</span></span>
<span><span class="co">#&gt; 7      S  182     85   -163      7    0   182     5</span></span>
<span><span class="co">#&gt; 8      R  193    115    -20     47   72   193    34</span></span>
<span><span class="co">#&gt; 9      R   84    213     71     53  156   250     3</span></span>
<span><span class="co">#&gt; 11     R   89    314    102     59  189   299     0</span></span>
<span><span class="co">#&gt; 12     R  105    226     92    118  160   215    10</span></span>
<span><span class="co">#&gt; 13     R   98    185     17     45  107   200     7</span></span>
<span><span class="co">#&gt; 15     R  164    286    103     13  207   300     1</span></span>
<span><span class="co">#&gt; 16     S   62    222     -4     28  119   227     1</span></span>
<span><span class="co">#&gt; 17     S  131    220     55     36  149   243    11</span></span>
<span><span class="co">#&gt; 19     R  144    182     21     52  109   198    16</span></span>
<span><span class="co">#&gt; 20     S  140    312    111      0  213   280     0</span></span>
<span><span class="co">#&gt; 22     R  259    320    157      5  261   307     0</span></span>
<span><span class="co">#&gt; 23     S  117    198     30     26  135   232     8</span></span>
<span><span class="co">#&gt; 25     S  154    276     94     16  197   291     2</span></span>
<span><span class="co">#&gt; 28     S   92    201     32     29  136   233     8</span></span>
<span><span class="co">#&gt; 30     R   17    192     24     92  114   231     3</span></span>
<span><span class="co">#&gt; 31     R  159    214     68     16  156   163    16</span></span>
<span><span class="co">#&gt; 33     R   94    231     83     74  163   243     3</span></span>
<span><span class="co">#&gt; 34     R  150     81    -21     49   49   166    29</span></span>
<span><span class="co">#&gt; 35     S  138    179     46     36  121   231    23</span></span>
<span><span class="co">#&gt; 36     S  194    269     94      2  191   211     2</span></span>
<span><span class="co">#&gt; 37     R  120    209    105     93  156   199     5</span></span>
<span><span class="co">#&gt; 38     R  111    232     91     78  166   236     2</span></span>
<span><span class="co">#&gt; 39     R   79    211     71     87  147   235    12</span></span>
<span><span class="co">#&gt; 41     S  101    223     89     70  167   253     1</span></span>
<span><span class="co">#&gt; 43     S  159    304    133      4  215   276     0</span></span>
<span><span class="co">#&gt; 44     R   98    278     93    124  184   267    12</span></span>
<span><span class="co">#&gt; 46     S  180    255     76      3  178   198     2</span></span>
<span><span class="co">#&gt; 47     S   35    232     42    114  143   248     3</span></span>
<span><span class="co">#&gt; 48     R  186     83   -107     30   26   186    23</span></span>
<span><span class="co">#&gt; 49     S  170    235     99      8  171   183     8</span></span>
<span><span class="co">#&gt; 50     S  202    287    101      5  204   227     5</span></span>
<span><span class="co">#&gt; 52     S   84    242     75     58  167   257     8</span></span>
<span><span class="co">#&gt; 53     R  129    265     40     23  178   276     9</span></span>
<span><span class="co">#&gt; 54     R   -8    194     22     87  116   236     2</span></span>
<span><span class="co">#&gt; 55     R  205    142      8     56  100   222    30</span></span>
<span><span class="co">#&gt; 58     S  165    238     94      6  178   195     6</span></span>
<span><span class="co">#&gt; 60     S  179    270     81      6  187   203     5</span></span>
<span><span class="co">#&gt; 61     S   64    265     69     83  177   293     1</span></span>
<span><span class="co">#&gt; 62     R  187    235     99     43  174   241    10</span></span>
<span><span class="co">#&gt; 63     S  154    214     70     18  154   158    18</span></span>
<span><span class="co">#&gt; 64     S  207    144      8     66  100   224    32</span></span>
<span><span class="co">#&gt; 66     S  211    194     33     23  129   233    22</span></span>
<span><span class="co">#&gt; 69     R  115    154     13     57   98   177    29</span></span>
<span><span class="co">#&gt; 70     S   81    240     73     49  165   255     9</span></span>
<span><span class="co">#&gt; 72     S   -4    197      4     80  104   205     5</span></span>
<span><span class="co">#&gt; 75     R   91    210     16     66  130   221     6</span></span>
<span><span class="co">#&gt; 76     S   99    224     89     61  170   261     1</span></span>
<span><span class="co">#&gt; 77     R  120    186     41      7  128   147     7</span></span>
<span><span class="co">#&gt; 78     R   61    191     56     82  127   212    22</span></span>
<span><span class="co">#&gt; 79     R  186     88   -110     23   22   186    18</span></span>
<span><span class="co">#&gt; 81     R  184     78   -119     20   14   184    14</span></span>
<span><span class="co">#&gt; 82     S  276    319    220      1  268   289     0</span></span>
<span><span class="co">#&gt; 84     S  121    204     48     27  138   219     6</span></span>
<span><span class="co">#&gt; 85     S  136    202     73      8  140   148     8</span></span>
<span><span class="co">#&gt; 87     R  126    271    110    150  189   268     6</span></span>
<span><span class="co">#&gt; 89     R  154    262    127     25  193   237     1</span></span>
<span><span class="co">#&gt; 90     S   92    218     80     62  163   255     3</span></span>
<span><span class="co">#&gt; 92     S  210    268     96     14  206   309     2</span></span>
<span><span class="co">#&gt; 93     R  112    270    117    138  190   268     5</span></span>
<span><span class="co">#&gt; 95     S   93    206     46     43  139   235    27</span></span>
<span><span class="co">#&gt; 96     S   73    203     58     59  145   241     5</span></span>
<span><span class="co">#&gt; 97     S  188    261    105      6  193   212     3</span></span>
<span><span class="co">#&gt; 99     R   54    174     32    124  118   195     3</span></span>
<span><span class="co">#&gt; 100    R   92    240     92    120  172   257     3</span></span>
<span><span class="co">#&gt; 103    R  195    136     -1     46   89   212    33</span></span>
<span><span class="co">#&gt; 104    S  125    186     48     10  129   144    10</span></span>
<span><span class="co">#&gt; 105    S   84    223     63     73  158   250     4</span></span>
<span><span class="co">#&gt; 108    R  229    235    135    102  173   233    26</span></span>
<span><span class="co">#&gt; 109    R   96    190     51     49  139   227    11</span></span>
<span><span class="co">#&gt; 111    R   30    202     24     85  120   211     9</span></span>
<span><span class="co">#&gt; 112    S  232    234     61     11  178   280     3</span></span>
<span><span class="co">#&gt; 113    R   75    198     57    144  132   211    13</span></span>
<span><span class="co">#&gt; 114    R   66    208     54     76  139   229    19</span></span>
<span><span class="co">#&gt; 115    S  193    166     27     45  112   208    31</span></span>
<span><span class="co">#&gt; 117    R  158    206     57     23  146   169    19</span></span>
<span><span class="co">#&gt; 118    R   84    243     76     62  167   256     8</span></span>
<span><span class="co">#&gt; 119    R  317    342    108      2  247   332     1</span></span>
<span><span class="co">#&gt; 120    R  184    262     89      5  188   205     5</span></span>
<span><span class="co">#&gt; 121    R   76    185     19     44  117   211    12</span></span>
<span><span class="co">#&gt; 124    R  121    194    109     26  149   250     9</span></span>
<span><span class="co">#&gt; 125    R   87    169      4     53   95   190     7</span></span>
<span><span class="co">#&gt; 126    R  151    189     27     42  117   210     9</span></span>
<span><span class="co">#&gt; 128    R   63    224     60    128  152   244     8</span></span>
<span><span class="co">#&gt; 129    S  132    192     54     15  137   148    15</span></span>
<span><span class="co">#&gt; 131    R  120    232    112    140  175   234     1</span></span>
<span><span class="co">#&gt; 133    R  103    225     94     64  173   262     1</span></span>
<span><span class="co">#&gt; 134    R  109    198     15     47  116   211    11</span></span>
<span><span class="co">#&gt; 135    R   95    214     27     54  148   265     2</span></span>
<span><span class="co">#&gt; 138    R   95    214     27     54  148   265     2</span></span>
<span><span class="co">#&gt; 139    S   56    172     27    138  111   185     4</span></span>
<span><span class="co">#&gt; 140    R   36    206     28     69  125   216    11</span></span>
<span><span class="co">#&gt; 141    S  121    233    112     55  183   264     1</span></span>
<span><span class="co">#&gt; 142    S   88    244     48    108  155   240     4</span></span>
<span><span class="co">#&gt; 143    S  107    248    101     65  177   253     3</span></span>
<span><span class="co">#&gt; 144    S  115    227     95     85  163   228     2</span></span>
<span><span class="co">#&gt; 145    S  142    209     56      6  143   159     3</span></span>
<span><span class="co">#&gt; 146    R  184    262     89      5  188   205     5</span></span>
<span><span class="co">#&gt; 147    S  141    205     59      7  147   164     7</span></span>
<span><span class="co">#&gt; 148    S   76    206     63     82  147   236     3</span></span>
<span><span class="co">#&gt; 149    R  129    261     61     70  171   251     2</span></span>
<span><span class="co">#&gt; 150    R  199    275    109     19  205   219    19</span></span>
<span><span class="co">#&gt; 151    R  125    288     93      8  197   274     0</span></span>
<span><span class="co">#&gt; 153    S   76    205     62     66  148   244     3</span></span>
<span><span class="co">#&gt; 154    S  100    224     89     67  167   253     1</span></span>
<span><span class="co">#&gt; 156    R   91    217     80     64  163   256     2</span></span>
<span><span class="co">#&gt; 157    R  157    245     76      7  177   224     2</span></span>
<span><span class="co">#&gt; 158    S  104    287    103    124  193   281    13</span></span>
<span><span class="co">#&gt; 160    R  155    230     32     20  157   281     2</span></span>
<span><span class="co">#&gt; 164    S  120    226    100     85  164   225     2</span></span>
<span><span class="co">#&gt; 165    S  116    174     48     15  123   133    15</span></span>
<span><span class="co">#&gt; 166    S  133    221     61     45  152   244    13</span></span>
<span><span class="co">#&gt; 167    S   89    217     76     62  159   249     3</span></span>
<span><span class="co">#&gt; 168    R  111    229    103     58  179   267     1</span></span>
<span><span class="co">#&gt; 170    R   38    210     31     89  128   220     8</span></span>
<span><span class="co">#&gt; 171    S   76    204     62     68  148   244     4</span></span>
<span><span class="co">#&gt; 173    R  -17    226     10     70  119   242     1</span></span>
<span><span class="co">#&gt; 175    R   -5    136    -59     47   50   156     0</span></span>
<span><span class="co">#&gt; 176    S  100    225     91     60  172   264     1</span></span>
<span><span class="co">#&gt; 179    S  210    268     95     14  206   309     2</span></span>
<span><span class="co">#&gt; 180    R  105    221     96     79  167   251     2</span></span>
<span><span class="co">#&gt; 181    S  115    197     34     26  132   223     8</span></span>
<span><span class="co">#&gt; 182    R  198     85    -59     47   46   198    29</span></span>
<span><span class="co">#&gt; 183    R  260    313    133     13  251   314     2</span></span>
<span><span class="co">#&gt; 184    R  -31    136    -95     30   29   146     0</span></span>
<span><span class="co">#&gt; 186    R  156    261    130     26  193   234     1</span></span>
<span><span class="co">#&gt; 188    R  157    222     88      9  159   169     9</span></span>
<span><span class="co">#&gt; 189    S  128    334    105     24  227   334     0</span></span>
<span><span class="co">#&gt; 190    S  132    183     49     17  133   141    17</span></span>
<span><span class="co">#&gt; 191    S  172    276    119     26  201   278     0</span></span>
<span><span class="co">#&gt; 192    S   63    224     -2     27  121   229     1</span></span>
<span><span class="co">#&gt; 193    R   25    187     26     72  115   204    12</span></span>
<span><span class="co">#&gt; 194    S   77    234     70     56  161   250     9</span></span>
<span><span class="co">#&gt; 195    R  153    191     34     42  118   203    12</span></span>
<span><span class="co">#&gt; 197    S  171    253     99      9  183   196     9</span></span>
<span><span class="co">#&gt; 199    S   68    271     64     76  177   288     0</span></span>
<span><span class="co">#&gt; 200    S  176    251    104     11  187   208     5</span></span></code></pre></div>
<div class="sourceCode" id="cb14"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">knn.ST_S</span><span class="op">$</span><span class="va">`Test Data`</span> </span>
<span><span class="co">#&gt;     ST_S bio8 tmax10 tmin11 prec12 bio1 bio10 bio14</span></span>
<span><span class="co">#&gt; 1      S  103    197     26     72  119   238     2</span></span>
<span><span class="co">#&gt; 2      R   40    215     28     99  131   223    11</span></span>
<span><span class="co">#&gt; 10     S   79    206     66     57  151   247     3</span></span>
<span><span class="co">#&gt; 14     S   87    196     26     31  130   229    10</span></span>
<span><span class="co">#&gt; 18     R  187    261    104      6  193   212     3</span></span>
<span><span class="co">#&gt; 21     R   80    209     65    115  142   219    12</span></span>
<span><span class="co">#&gt; 24     S  162    224     85     10  161   172    10</span></span>
<span><span class="co">#&gt; 26     S  196    262     95     11  189   208    11</span></span>
<span><span class="co">#&gt; 27     S  170    248    102      6  187   206     6</span></span>
<span><span class="co">#&gt; 29     R   74    213     60    103  143   232     9</span></span>
<span><span class="co">#&gt; 32     R  128    176     -3     48   90   178     7</span></span>
<span><span class="co">#&gt; 40     R  233    278    135     48  176   241     8</span></span>
<span><span class="co">#&gt; 42     S  154    214     70     18  154   160    18</span></span>
<span><span class="co">#&gt; 45     S   33    149     47    185   99   171     6</span></span>
<span><span class="co">#&gt; 51     S   74    203     61     64  147   243     4</span></span>
<span><span class="co">#&gt; 56     S  135    234    114     85  182   247     2</span></span>
<span><span class="co">#&gt; 57     R   38    205     34     73  127   217    11</span></span>
<span><span class="co">#&gt; 59     R  104    287    108    137  192   274     6</span></span>
<span><span class="co">#&gt; 65     S   10    184     18     51  108   201     9</span></span>
<span><span class="co">#&gt; 67     R  145    194     56     69  138   213    27</span></span>
<span><span class="co">#&gt; 68     R  111    262    109    117  179   246     1</span></span>
<span><span class="co">#&gt; 71     R   74    213     60    103  143   232     9</span></span>
<span><span class="co">#&gt; 73     S  173    260     81      7  182   197     7</span></span>
<span><span class="co">#&gt; 74     R  150    250     89     35  180   254     2</span></span>
<span><span class="co">#&gt; 80     S   87    235     72     46  163   253     9</span></span>
<span><span class="co">#&gt; 83     S  177     68    -54     44   42   177    25</span></span>
<span><span class="co">#&gt; 86     S   87    235     72     46  163   253     9</span></span>
<span><span class="co">#&gt; 88     R  113    202     11     64   95   115     2</span></span>
<span><span class="co">#&gt; 91     R   87    196     26     31  130   228     9</span></span>
<span><span class="co">#&gt; 94     S  101    222     92     65  170   260     2</span></span>
<span><span class="co">#&gt; 98     S  184    262     89      5  188   205     5</span></span>
<span><span class="co">#&gt; 101    S   68    200     55     73  136   213     5</span></span>
<span><span class="co">#&gt; 102    R   98    147    -27     39   61   161    16</span></span>
<span><span class="co">#&gt; 106    R  176    251    104     11  187   208     5</span></span>
<span><span class="co">#&gt; 107    S  143    200     64      7  145   160     7</span></span>
<span><span class="co">#&gt; 110    S  149    273     90     35  189   267     2</span></span>
<span><span class="co">#&gt; 116    R  114    222    127    127  173   237     1</span></span>
<span><span class="co">#&gt; 122    R  104    287    103    124  193   281    13</span></span>
<span><span class="co">#&gt; 123    S  108    276    115    144  191   271     6</span></span>
<span><span class="co">#&gt; 127    R  119    254     98     83  186   267     1</span></span>
<span><span class="co">#&gt; 130    S  181    243     76     12  174   194     9</span></span>
<span><span class="co">#&gt; 132    R  142    209     54     10  148   164     9</span></span>
<span><span class="co">#&gt; 136    S  144    191     14     49  108   194    12</span></span>
<span><span class="co">#&gt; 137    R  218    112    -41     36   67   218    21</span></span>
<span><span class="co">#&gt; 152    R  128    281     91     30  202   288     0</span></span>
<span><span class="co">#&gt; 155    R  177    188     48     16  135   238    13</span></span>
<span><span class="co">#&gt; 159    S  107    256    104     20  181   258     1</span></span>
<span><span class="co">#&gt; 161    S  165    232     49      7  158   178     5</span></span>
<span><span class="co">#&gt; 162    R   19    184     18     76  109   199    13</span></span>
<span><span class="co">#&gt; 163    R  100    187     13     44  108   198     9</span></span>
<span><span class="co">#&gt; 169    S  115    197     38     27  132   218     8</span></span>
<span><span class="co">#&gt; 172    S   96    209    104     40  152   219    31</span></span>
<span><span class="co">#&gt; 174    R   62    196     44     45  134   227     9</span></span>
<span><span class="co">#&gt; 177    R  108    188     30     48  117   209    12</span></span>
<span><span class="co">#&gt; 178    R    5    186    -26     49   99   202     0</span></span>
<span><span class="co">#&gt; 185    R   79    194     77    104  138   220    22</span></span>
<span><span class="co">#&gt; 187    R  178    125     -5     43   73   178    31</span></span>
<span><span class="co">#&gt; 196    S  101    223     92     82  161   229     4</span></span>
<span><span class="co">#&gt; 198    R   89    197     28     31  131   229     9</span></span></code></pre></div>
</div>
<div class="section level3">
<h3 id="binary-classification-imbalanced-data">Binary classification : imbalanced data<a class="anchor" aria-label="anchor" href="#binary-classification-imbalanced-data"></a>
</h3>
<p><code>BarleyRNO</code> dataset is designed for modeling morphological
characteristics of barley, specifically focusing on the response
classification of barley Kernel row number type under climate
influences. The response variable RNO categorizes barley into the
following levels: <br><strong>1 (Six-rowed):</strong> Represents barley
genotypes with six distinct rows of kernels on the spike, typically
associated with higher grain yields. <br><strong>2
(Two-rowed):</strong> Represents barley genotypes with two distinct rows
of kernels on the spike, often preferred for malting and brewing due to
uniformity in kernel size. <br></p>
<p><strong>3 (Two-rowed - rudimentary florets):</strong> Represents
two-rowed barley with underdeveloped or rudimentary florets. <br></p>
<p><strong>4 (Irregular lateral florets):</strong> Represents barley
genotypes with irregularly developed lateral florets on the spike. <br><strong>5 (Irregular: 2 and 6 rows):</strong> Represents heterogeneous
genotypes showing spikes with both two-rowed and six-rowed
characteristics. <br><strong>10 (Heterogeneous):</strong> Represents a
genetically diverse group with mixed spike structures. <br></p>
<p>For this example, we use only the first 2 categories :
<code>six-rowed</code> and <code>two-rowed</code>.</p>
<p>These levels are critical for building and validating models to
classify barley kernel row number based on environmental predictors and
possibily genetic predictors, aiding in breeding programs and genotype
selection.</p>
<div class="sourceCode" id="cb15"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span></span>
<span><span class="co"># Load sample data imbalanced data for binary classification  </span></span>
<span></span>
<span><span class="va">BarleyRNOWC</span> <span class="op">&lt;-</span> <span class="fu">icardaFIGSr</span><span class="fu">::</span><span class="va"><a href="../reference/BarleyRNOWC.html">BarleyRNOWC</a></span></span>
<span></span>
<span><span class="co"># lets sample BarleyRNOWC dataset </span></span>
<span></span>
<span><span class="va">random_cols</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/sample.html" class="external-link">sample</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/colnames.html" class="external-link">colnames</a></span><span class="op">(</span><span class="va">BarleyRNOWC</span><span class="op">)</span>, <span class="fl">7</span><span class="op">)</span></span>
<span> </span>
<span><span class="co"># Note : we can still use the whole dataset for model training and testing.</span></span>
<span></span>
<span><span class="co"># Select Y and predictors.</span></span>
<span> <span class="va">BarleyRNOWC_sample</span> <span class="op">&lt;-</span> <span class="va">BarleyRNOWC</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html" class="external-link">%&gt;%</a></span></span>
<span>   <span class="fu">dplyr</span><span class="fu">::</span><span class="fu"><a href="https://dplyr.tidyverse.org/reference/select.html" class="external-link">select</a></span><span class="op">(</span><span class="va">RNO</span>, <span class="fu"><a href="https://tidyselect.r-lib.org/reference/all_of.html" class="external-link">all_of</a></span><span class="op">(</span><span class="va">random_cols</span><span class="op">)</span><span class="op">)</span></span>
<span> </span>
<span><span class="co"># Check results </span></span>
<span> <span class="va">BarleyRNOWC_sample</span></span>
<span><span class="co">#&gt;     RNO  tmax_11    bio_15  tmin_06 prec_03 prec_08  tmax_03 bio_18</span></span>
<span><span class="co">#&gt; 1     1 30.09200  65.05665 26.52000      26       7 30.18800     26</span></span>
<span><span class="co">#&gt; 2     1 22.22000  97.68206 17.95200      12       1 19.35200      3</span></span>
<span><span class="co">#&gt; 3     1 -4.12800 105.32485 10.56400      10     139 -0.99600    390</span></span>
<span><span class="co">#&gt; 4     1 -4.12800 105.32485 10.56400      10     139 -0.99600    390</span></span>
<span><span class="co">#&gt; 5     1 15.13600  81.09148 20.10800      44     126 12.61200    449</span></span>
<span><span class="co">#&gt; 6     1 11.08800 109.93213 16.84800      20     160 10.93600    498</span></span>
<span><span class="co">#&gt; 7     1 20.03200  85.06351 18.66000      18     219 24.60800    621</span></span>
<span><span class="co">#&gt; 8     1  2.48000  92.24405  4.56800      11      83  3.90000    230</span></span>
<span><span class="co">#&gt; 9     1  6.99200 103.33441 12.21200      12     124  7.90800    312</span></span>
<span><span class="co">#&gt; 10    1  5.59200  97.85579 16.80000      16     155  6.54800    418</span></span>
<span><span class="co">#&gt; 11    1 -4.12800 105.32485 10.56400      10     139 -0.99600    390</span></span>
<span><span class="co">#&gt; 12    1 11.08800 109.93213 16.84800      20     160 10.93600    498</span></span>
<span><span class="co">#&gt; 13    1  5.59200  97.85579 16.80000      16     155  6.54800    418</span></span>
<span><span class="co">#&gt; 14    1 19.62400  90.98979 16.02800      62       2 16.09200      9</span></span>
<span><span class="co">#&gt; 15    1 16.80800  47.55249 20.60400      68     146 12.72800    421</span></span>
<span><span class="co">#&gt; 16    1  6.99200 103.33441 12.21200      12     124  7.90800    312</span></span>
<span><span class="co">#&gt; 17    1 15.26000  36.74965 13.36000      30       7 15.16000     36</span></span>
<span><span class="co">#&gt; 18    1 11.08800 109.93213 16.84800      20     160 10.93600    498</span></span>
<span><span class="co">#&gt; 19    1 16.80800  47.55249 20.60400      68     146 12.72800    421</span></span>
<span><span class="co">#&gt; 20    1  5.59200  97.85579 16.80000      16     155  6.54800    418</span></span>
<span><span class="co">#&gt; 21    1 16.38800  44.06684 18.88800      60      65 18.06400    221</span></span>
<span><span class="co">#&gt; 22    1 11.08800 109.93213 16.84800      20     160 10.93600    498</span></span>
<span><span class="co">#&gt; 23    1 14.65200  81.31908 20.06000      30     132 14.04800    382</span></span>
<span><span class="co">#&gt; 24    1 17.56800  48.06286 20.52000     129     131 14.62400    505</span></span>
<span><span class="co">#&gt; 25    1 15.18400  58.32013 12.97200      89      11 13.76800     58</span></span>
<span><span class="co">#&gt; 26    1 11.08800 109.93213 16.84800      20     160 10.93600    498</span></span>
<span><span class="co">#&gt; 27    1 10.69600 142.14531 19.14400       8     151 12.38000    381</span></span>
<span><span class="co">#&gt; 28    1  6.99200 103.33441 12.21200      12     124  7.90800    312</span></span>
<span><span class="co">#&gt; 29    1 11.08800 109.93213 16.84800      20     160 10.93600    498</span></span>
<span><span class="co">#&gt; 30    1 11.08800 109.93213 16.84800      20     160 10.93600    498</span></span>
<span><span class="co">#&gt; 31    1 11.08800 109.93213 16.84800      20     160 10.93600    498</span></span>
<span><span class="co">#&gt; 32    1 24.30000  81.19894 23.08000      38      15 24.52000     32</span></span>
<span><span class="co">#&gt; 33    1  5.59200  97.85579 16.80000      16     155  6.54800    418</span></span>
<span><span class="co">#&gt; 34    1 27.54000  67.51161 26.39200      34      20 27.72800     37</span></span>
<span><span class="co">#&gt; 35    1 20.02400  75.76439 19.86000      12       0 19.18800      0</span></span>
<span><span class="co">#&gt; 36    1  6.99200 103.33441 12.21200      12     124  7.90800    312</span></span>
<span><span class="co">#&gt; 37    1 -4.12800 105.32485 10.56400      10     139 -0.99600    390</span></span>
<span><span class="co">#&gt; 38    1  6.99200 103.33441 12.21200      12     124  7.90800    312</span></span>
<span><span class="co">#&gt; 39    1 20.99600  68.61182 19.58400      71       8 18.94400     32</span></span>
<span><span class="co">#&gt; 40    1 24.32400 102.93157 18.31600      17       0 21.66000      0</span></span>
<span><span class="co">#&gt; 41    1  6.99200 103.33441 12.21200      12     124  7.90800    312</span></span>
<span><span class="co">#&gt; 42    1 -4.12800 105.32485 10.56400      10     139 -0.99600    390</span></span>
<span><span class="co">#&gt; 43    1 11.08800 109.93213 16.84800      20     160 10.93600    498</span></span>
<span><span class="co">#&gt; 44    1 11.08800 109.93213 16.84800      20     160 10.93600    498</span></span>
<span><span class="co">#&gt; 45    1  6.99200 103.33441 12.21200      12     124  7.90800    312</span></span>
<span><span class="co">#&gt; 46    1 11.08800 109.93213 16.84800      20     160 10.93600    498</span></span>
<span><span class="co">#&gt; 47    1 10.69600 142.14531 19.14400       8     151 12.38000    381</span></span>
<span><span class="co">#&gt; 48    1 15.62800  58.71524 20.99600      59     132 13.52400    433</span></span>
<span><span class="co">#&gt; 49    1 11.08800 109.93213 16.84800      20     160 10.93600    498</span></span>
<span><span class="co">#&gt; 50    1 11.08800 109.93213 16.84800      20     160 10.93600    498</span></span>
<span><span class="co">#&gt; 51    1 11.08800 109.93213 16.84800      20     160 10.93600    498</span></span>
<span><span class="co">#&gt; 52    1  6.99200 103.33441 12.21200      12     124  7.90800    312</span></span>
<span><span class="co">#&gt; 53    1 11.08800 109.93213 16.84800      20     160 10.93600    498</span></span>
<span><span class="co">#&gt; 54    1 17.56800  48.06286 20.52000     129     131 14.62400    505</span></span>
<span><span class="co">#&gt; 55    1 17.56800  48.06286 20.52000     129     131 14.62400    505</span></span>
<span><span class="co">#&gt; 56    1 11.08800 109.93213 16.84800      20     160 10.93600    498</span></span>
<span><span class="co">#&gt; 57    1 11.08800 109.93213 16.84800      20     160 10.93600    498</span></span>
<span><span class="co">#&gt; 58    1  6.99200 103.33441 12.21200      12     124  7.90800    312</span></span>
<span><span class="co">#&gt; 59    1 11.08800 109.93213 16.84800      20     160 10.93600    498</span></span>
<span><span class="co">#&gt; 60    1 14.65200  81.31908 20.06000      30     132 14.04800    382</span></span>
<span><span class="co">#&gt; 61    1 19.62400  90.98979 16.02800      62       2 16.09200      9</span></span>
<span><span class="co">#&gt; 62    1 17.56800  48.06286 20.52000     129     131 14.62400    505</span></span>
<span><span class="co">#&gt; 63    1 15.95600  85.60078 20.70400      24     204 16.16000    586</span></span>
<span><span class="co">#&gt; 64    1 19.89091  70.98363 16.28182      78       6 18.14545     57</span></span>
<span><span class="co">#&gt; 65    1 16.60800  47.01530 21.44400     127     123 13.98800    439</span></span>
<span><span class="co">#&gt; 66    1 15.95600  85.60078 20.70400      24     204 16.16000    586</span></span>
<span><span class="co">#&gt; 67    1 11.08800 109.93213 16.84800      20     160 10.93600    498</span></span>
<span><span class="co">#&gt; 68    1 20.87200  41.11276 23.98800      14       5 21.64400     13</span></span>
<span><span class="co">#&gt; 69    1 11.08800 109.93213 16.84800      20     160 10.93600    498</span></span>
<span><span class="co">#&gt; 70    1  6.99200 103.33441 12.21200      12     124  7.90800    312</span></span>
<span><span class="co">#&gt; 71    1 11.08800 109.93213 16.84800      20     160 10.93600    498</span></span>
<span><span class="co">#&gt; 72    1 -5.04000  36.43435 -1.51200      23      18 -7.98000     55</span></span>
<span><span class="co">#&gt; 73    1 21.08000  85.80009 19.78800     114       4 18.31600     29</span></span>
<span><span class="co">#&gt; 74    1 11.08800 109.93213 16.84800      20     160 10.93600    498</span></span>
<span><span class="co">#&gt; 75    1 17.56800  48.06286 20.52000     129     131 14.62400    505</span></span>
<span><span class="co">#&gt; 76    1 22.05200  97.64330 17.77600      12       1 19.15200      3</span></span>
<span><span class="co">#&gt; 77    1 10.69600 142.14531 19.14400       8     151 12.38000    381</span></span>
<span><span class="co">#&gt; 78    1 15.62800  58.71524 20.99600      59     132 13.52400    433</span></span>
<span><span class="co">#&gt; 79    1 15.13600  81.09148 20.10800      44     126 12.61200    449</span></span>
<span><span class="co">#&gt; 80    1 22.33600 100.03368 18.02400      12       1 19.45200      2</span></span>
<span><span class="co">#&gt; 81    1 11.08800 109.93213 16.84800      20     160 10.93600    498</span></span>
<span><span class="co">#&gt; 82    1 15.13600  81.09148 20.10800      44     126 12.61200    449</span></span>
<span><span class="co">#&gt; 83    1 15.13600  81.09148 20.10800      44     126 12.61200    449</span></span>
<span><span class="co">#&gt; 84    1 15.62800  58.71524 20.99600      59     132 13.52400    433</span></span>
<span><span class="co">#&gt; 85    1 30.53200  79.71966 28.16400      28       8 29.60800     21</span></span>
<span><span class="co">#&gt; 86    1 15.95600  85.60078 20.70400      24     204 16.16000    586</span></span>
<span><span class="co">#&gt; 87    1 10.20400  64.41352  8.52000      46      12  8.50000     30</span></span>
<span><span class="co">#&gt; 88    1 11.08800 109.93213 16.84800      20     160 10.93600    498</span></span>
<span><span class="co">#&gt; 89    1 -4.12800 105.32485 10.56400      10     139 -0.99600    390</span></span>
<span><span class="co">#&gt; 90    1 23.99200 103.69400 18.38000       9       0 21.29200      1</span></span>
<span><span class="co">#&gt; 91    1  6.99200 103.33441 12.21200      12     124  7.90800    312</span></span>
<span><span class="co">#&gt; 92    1 11.08800 109.93213 16.84800      20     160 10.93600    498</span></span>
<span><span class="co">#&gt; 93    1 11.08800 109.93213 16.84800      20     160 10.93600    498</span></span>
<span><span class="co">#&gt; 94    1  6.99200 103.33441 12.21200      12     124  7.90800    312</span></span>
<span><span class="co">#&gt; 95    1 11.44000  60.87855 12.02000      17      17 10.20800     40</span></span>
<span><span class="co">#&gt; 96    1  2.48000  92.24405  4.56800      11      83  3.90000    230</span></span>
<span><span class="co">#&gt; 97    1 11.08800 109.93213 16.84800      20     160 10.93600    498</span></span>
<span><span class="co">#&gt; 98    1 16.60800  47.01530 21.44400     127     123 13.98800    439</span></span>
<span><span class="co">#&gt; 99    1 15.95600  85.60078 20.70400      24     204 16.16000    586</span></span>
<span><span class="co">#&gt; 100   1 17.56800  48.06286 20.52000     129     131 14.62400    505</span></span>
<span><span class="co">#&gt; 101   1 19.65200  76.46056 19.66000      14       0 18.70800      0</span></span>
<span><span class="co">#&gt; 102   1 -4.12800 105.32485 10.56400      10     139 -0.99600    390</span></span>
<span><span class="co">#&gt; 103   1  2.48000  92.24405  4.56800      11      83  3.90000    230</span></span>
<span><span class="co">#&gt; 104   1 11.08800 109.93213 16.84800      20     160 10.93600    498</span></span>
<span><span class="co">#&gt; 105   1 29.73600  84.48630 26.80800      27       4 29.80800     19</span></span>
<span><span class="co">#&gt; 106   1 15.62800  58.71524 20.99600      59     132 13.52400    433</span></span>
<span><span class="co">#&gt; 107   1 14.65200  81.31908 20.06000      30     132 14.04800    382</span></span>
<span><span class="co">#&gt; 108   1 12.53600  32.68545 12.19200      50      18 12.46000     53</span></span>
<span><span class="co">#&gt; 109   1 22.72000  77.00456 21.03200      40      19 21.94000     48</span></span>
<span><span class="co">#&gt; 110   1 11.08800 109.93213 16.84800      20     160 10.93600    498</span></span>
<span><span class="co">#&gt; 111   1  5.59200  97.85579 16.80000      16     155  6.54800    418</span></span>
<span><span class="co">#&gt; 112   1 17.56800  48.06286 20.52000     129     131 14.62400    505</span></span>
<span><span class="co">#&gt; 113   1  6.99200 103.33441 12.21200      12     124  7.90800    312</span></span>
<span><span class="co">#&gt; 114   1 15.62800  58.71524 20.99600      59     132 13.52400    433</span></span>
<span><span class="co">#&gt; 115   1  2.48000  92.24405  4.56800      11      83  3.90000    230</span></span>
<span><span class="co">#&gt; 116   1 14.65200  81.31908 20.06000      30     132 14.04800    382</span></span>
<span><span class="co">#&gt; 117   1 30.02000  88.46160 27.07200      29       5 29.82000     12</span></span>
<span><span class="co">#&gt; 118   1 21.48000  90.34368 18.05200     141       1 18.56000     10</span></span>
<span><span class="co">#&gt; 119   1 -4.12800 105.32485 10.56400      10     139 -0.99600    390</span></span>
<span><span class="co">#&gt; 120   1 11.08800 109.93213 16.84800      20     160 10.93600    498</span></span>
<span><span class="co">#&gt; 121   1 24.01200  94.39568 20.79200       9       0 22.74400      0</span></span>
<span><span class="co">#&gt; 122   1  5.59200  97.85579 16.80000      16     155  6.54800    418</span></span>
<span><span class="co">#&gt; 123   1 17.79200  54.78045 15.80400      25      20 16.52800     50</span></span>
<span><span class="co">#&gt; 124   1 17.56800  48.06286 20.52000     129     131 14.62400    505</span></span>
<span><span class="co">#&gt; 125   1 12.53600  32.68545 12.19200      50      18 12.46000     53</span></span>
<span><span class="co">#&gt; 126   1 10.69600 142.14531 19.14400       8     151 12.38000    381</span></span>
<span><span class="co">#&gt; 127   1  6.99200 103.33441 12.21200      12     124  7.90800    312</span></span>
<span><span class="co">#&gt; 128   1 22.00000  92.05145 17.46000      11       0 19.14800      2</span></span>
<span><span class="co">#&gt; 129   1 15.13600  81.09148 20.10800      44     126 12.61200    449</span></span>
<span><span class="co">#&gt; 130   1 15.95600  85.60078 20.70400      24     204 16.16000    586</span></span>
<span><span class="co">#&gt; 131   1 20.46800  89.26852 18.39200      29       5 19.91200     19</span></span>
<span><span class="co">#&gt; 132   1  6.99200 103.33441 12.21200      12     124  7.90800    312</span></span>
<span><span class="co">#&gt; 133   1 20.03200  85.06351 18.66000      18     219 24.60800    621</span></span>
<span><span class="co">#&gt; 134   1  6.99200 103.33441 12.21200      12     124  7.90800    312</span></span>
<span><span class="co">#&gt; 135   1 11.08800 109.93213 16.84800      20     160 10.93600    498</span></span>
<span><span class="co">#&gt; 136   1 20.03200  85.06351 18.66000      18     219 24.60800    621</span></span>
<span><span class="co">#&gt; 137   1 11.08800 109.93213 16.84800      20     160 10.93600    498</span></span>
<span><span class="co">#&gt; 138   1  6.99200 103.33441 12.21200      12     124  7.90800    312</span></span>
<span><span class="co">#&gt; 139   1 15.13600  81.09148 20.10800      44     126 12.61200    449</span></span>
<span><span class="co">#&gt; 140   1 22.62400 100.10287 18.30400      12       1 19.75200      2</span></span>
<span><span class="co">#&gt; 141   1 10.69600 142.14531 19.14400       8     151 12.38000    381</span></span>
<span><span class="co">#&gt; 142   1 24.01200  94.39568 20.79200       9       0 22.74400      0</span></span>
<span><span class="co">#&gt; 143   1 17.56800  48.06286 20.52000     129     131 14.62400    505</span></span>
<span><span class="co">#&gt; 144   1 15.13600  81.09148 20.10800      44     126 12.61200    449</span></span>
<span><span class="co">#&gt; 145   1  6.99200 103.33441 12.21200      12     124  7.90800    312</span></span>
<span><span class="co">#&gt; 146   1  2.48000  92.24405  4.56800      11      83  3.90000    230</span></span>
<span><span class="co">#&gt; 147   1 15.13600  81.09148 20.10800      44     126 12.61200    449</span></span>
<span><span class="co">#&gt; 148   1 -4.12800 105.32485 10.56400      10     139 -0.99600    390</span></span>
<span><span class="co">#&gt; 149   1 15.95600  85.60078 20.70400      24     204 16.16000    586</span></span>
<span><span class="co">#&gt; 150   1 30.02000  88.46160 27.07200      29       5 29.82000     12</span></span>
<span><span class="co">#&gt; 151   1  2.48000  92.24405  4.56800      11      83  3.90000    230</span></span>
<span><span class="co">#&gt; 152   1 31.90800 149.30829 27.45600       3      78 33.96000     94</span></span>
<span><span class="co">#&gt; 153   1 17.56800  48.06286 20.52000     129     131 14.62400    505</span></span>
<span><span class="co">#&gt; 154   1 11.08800 109.93213 16.84800      20     160 10.93600    498</span></span>
<span><span class="co">#&gt; 155   1 15.13600  81.09148 20.10800      44     126 12.61200    449</span></span>
<span><span class="co">#&gt; 156   1  6.99200 103.33441 12.21200      12     124  7.90800    312</span></span>
<span><span class="co">#&gt; 157   1 15.95600  85.60078 20.70400      24     204 16.16000    586</span></span>
<span><span class="co">#&gt; 158   1  6.99200 103.33441 12.21200      12     124  7.90800    312</span></span>
<span><span class="co">#&gt; 159   1 11.08800 109.93213 16.84800      20     160 10.93600    498</span></span>
<span><span class="co">#&gt; 160   1 11.08800 109.93213 16.84800      20     160 10.93600    498</span></span>
<span><span class="co">#&gt; 161   1 17.56800  48.06286 20.52000     129     131 14.62400    505</span></span>
<span><span class="co">#&gt; 162   1 11.08800 109.93213 16.84800      20     160 10.93600    498</span></span>
<span><span class="co">#&gt; 163   1  6.99200 103.33441 12.21200      12     124  7.90800    312</span></span>
<span><span class="co">#&gt; 164   1 11.62400  38.53807 11.43200      49      14 11.61200     47</span></span>
<span><span class="co">#&gt; 165   1 15.44400  33.49338 14.96400      49      16 15.60000     50</span></span>
<span><span class="co">#&gt; 166   1 -1.97200  45.39811  1.24000      32      15 -5.29200     40</span></span>
<span><span class="co">#&gt; 167   1 11.08800 109.93213 16.84800      20     160 10.93600    498</span></span>
<span><span class="co">#&gt; 168   1 17.56800  48.06286 20.52000     129     131 14.62400    505</span></span>
<span><span class="co">#&gt; 169   1  6.99200 103.33441 12.21200      12     124  7.90800    312</span></span>
<span><span class="co">#&gt; 170   1 18.14000  90.19624 15.29600      81       4 14.64800     17</span></span>
<span><span class="co">#&gt; 171   2 15.62800  58.71524 20.99600      59     132 13.52400    433</span></span>
<span><span class="co">#&gt; 172   2 29.94800  82.61971 26.82800      28       5 30.07200     20</span></span>
<span><span class="co">#&gt; 173   2 19.90800  81.88865 20.23600      64       0 17.28400      3</span></span>
<span><span class="co">#&gt; 174   2 22.71600  97.99079 18.30000      13       1 19.72400      3</span></span>
<span><span class="co">#&gt; 175   2 19.04400  79.07721 17.74400      14       0 17.86000      0</span></span>
<span><span class="co">#&gt; 176   2 15.63200  47.71310 14.27600      63      11 14.83600     44</span></span>
<span><span class="co">#&gt; 177   2 19.60000  87.20588 17.88000      42       0 17.42400      2</span></span>
<span><span class="co">#&gt; 178   2 17.14800  37.05869 15.62800      51      13 16.64400     48</span></span>
<span><span class="co">#&gt; 179   2 20.03200  85.06351 18.66000      18     219 24.60800    621</span></span>
<span><span class="co">#&gt; 180   2  5.59200  97.85579 16.80000      16     155  6.54800    418</span></span>
<span><span class="co">#&gt; 181   2 16.80800  47.55249 20.60400      68     146 12.72800    421</span></span>
<span><span class="co">#&gt; 182   2 16.80800  47.55249 20.60400      68     146 12.72800    421</span></span>
<span><span class="co">#&gt; 183   2 16.80800  47.55249 20.60400      68     146 12.72800    421</span></span>
<span><span class="co">#&gt; 184   2 17.56800  48.06286 20.52000     129     131 14.62400    505</span></span>
<span><span class="co">#&gt; 185   2 18.79600  86.59062 19.55200      82       1 16.99600      9</span></span>
<span><span class="co">#&gt; 186   2 17.56800  48.06286 20.52000     129     131 14.62400    505</span></span>
<span><span class="co">#&gt; 187   2 18.58000  76.19782 16.73600      13       0 17.39600      0</span></span>
<span><span class="co">#&gt; 188   2 17.56800  48.06286 20.52000     129     131 14.62400    505</span></span>
<span><span class="co">#&gt; 189   2 20.03200  85.06351 18.66000      18     219 24.60800    621</span></span>
<span><span class="co">#&gt; 190   2 22.60400  98.53925 18.22800      12       1 19.65200      3</span></span>
<span><span class="co">#&gt; 191   2  6.99200 103.33441 12.21200      12     124  7.90800    312</span></span>
<span><span class="co">#&gt; 192   2 16.80800  47.55249 20.60400      68     146 12.72800    421</span></span>
<span><span class="co">#&gt; 193   2 20.02400  75.76439 19.86000      12       0 19.18800      0</span></span>
<span><span class="co">#&gt; 194   2 30.74800  91.70721 26.63600      34       5 29.74800     15</span></span>
<span><span class="co">#&gt; 195   2  6.99200 103.33441 12.21200      12     124  7.90800    312</span></span>
<span><span class="co">#&gt; 196   2 22.79200  91.51672 17.80400      11       0 21.24400      0</span></span>
<span><span class="co">#&gt; 197   2 19.42000  86.17073 15.56400      20       0 18.23200      0</span></span>
<span><span class="co">#&gt; 198   2 18.20400  83.23743 16.35200      38       0 16.63200      1</span></span>
<span><span class="co">#&gt; 199   2 19.87200  75.85405 20.31200      16       0 18.94000      0</span></span>
<span><span class="co">#&gt; 200   2 19.41200  76.84998 18.75200      13       0 18.44400      0</span></span></code></pre></div>
<div class="sourceCode" id="cb16"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Count classes for data imbalance check</span></span>
<span><span class="va">BarleyRNOWC_sample</span><span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html" class="external-link">%&gt;%</a></span></span>
<span>  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/count.html" class="external-link">count</a></span><span class="op">(</span><span class="va">RNO</span><span class="op">)</span></span>
<span><span class="co">#&gt;   RNO   n</span></span>
<span><span class="co">#&gt; 1   1 170</span></span>
<span><span class="co">#&gt; 2   2  30</span></span></code></pre></div>
<div class="sourceCode" id="cb17"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span></span>
<span><span class="co">## Binary classification of RNO </span></span>
<span><span class="va">rf.RNO</span> <span class="op">&lt;-</span> <span class="fu">icardaFIGSr</span><span class="fu">::</span><span class="fu"><a href="../reference/tuneTrain.html">tuneTrain</a></span><span class="op">(</span>data <span class="op">=</span> <span class="va">BarleyRNOWC_sample</span>,</span>
<span>                      y <span class="op">=</span>  <span class="st">'RNO'</span>,</span>
<span>                      method <span class="op">=</span> <span class="st">'rf'</span>,</span>
<span>                      summary <span class="op">=</span> <span class="va">multiClassSummary</span>,</span>
<span>                      imbalanceMethod <span class="op">=</span><span class="st">"up"</span>, </span>
<span>                      imbalanceThreshold <span class="op">=</span> <span class="fl">0.2</span>,</span>
<span>                      process <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="st">"scale"</span>,<span class="st">"center"</span><span class="op">)</span>,</span>
<span>                     classProbs <span class="op">=</span> <span class="cn">TRUE</span>,</span>
<span>                     repeats <span class="op">=</span> <span class="fl">3</span><span class="op">)</span></span>
<span><span class="co">#&gt; note: only 6 unique complexity parameters in default grid. Truncating the grid to 6 .</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Random Forest </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; 140 samples</span></span>
<span><span class="co">#&gt;   7 predictor</span></span>
<span><span class="co">#&gt;   2 classes: 'Cl_1', 'Cl_2' </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Pre-processing: scaled (7), centered (7) </span></span>
<span><span class="co">#&gt; Resampling: Cross-Validated (10 fold, repeated 3 times) </span></span>
<span><span class="co">#&gt; Summary of sample sizes: 126, 126, 126, 126, 126, 126, ... </span></span>
<span><span class="co">#&gt; Addtional sampling using up-sampling prior to pre-processing</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Resampling results across tuning parameters:</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;   mtry  logLoss   AUC        prAUC      Accuracy   Kappa      F1       </span></span>
<span><span class="co">#&gt;   2     1.351967  0.7435396  0.4285756  0.7796093  0.2839066  0.8616300</span></span>
<span><span class="co">#&gt;   3     1.316165  0.7359007  0.4234439  0.7712332  0.2636587  0.8532896</span></span>
<span><span class="co">#&gt;   4     1.598478  0.7445707  0.4219979  0.7697192  0.2677371  0.8547945</span></span>
<span><span class="co">#&gt;   5     1.345545  0.7487584  0.4214455  0.7862271  0.3024619  0.8649212</span></span>
<span><span class="co">#&gt;   6     1.601229  0.7247896  0.4285286  0.7649817  0.2316640  0.8517068</span></span>
<span><span class="co">#&gt;   7     1.683125  0.7121002  0.4058053  0.7790110  0.3142004  0.8578083</span></span>
<span><span class="co">#&gt;   Sensitivity  Specificity  Pos_Pred_Value  Neg_Pred_Value  Precision</span></span>
<span><span class="co">#&gt;   0.8267677    0.5166667    0.9080107       0.3658730       0.9080107</span></span>
<span><span class="co">#&gt;   0.8169192    0.5111111    0.9101368       0.3514286       0.9101368</span></span>
<span><span class="co">#&gt;   0.8181818    0.5000000    0.9035274       0.3658730       0.9035274</span></span>
<span><span class="co">#&gt;   0.8318182    0.5277778    0.9124063       0.3980952       0.9124063</span></span>
<span><span class="co">#&gt;   0.8181818    0.4666667    0.8994466       0.3286508       0.8994466</span></span>
<span><span class="co">#&gt;   0.8202020    0.5500000    0.9130392       0.4053175       0.9130392</span></span>
<span><span class="co">#&gt;   Recall     Detection_Rate  Balanced_Accuracy</span></span>
<span><span class="co">#&gt;   0.8267677  0.7030281       0.6717172        </span></span>
<span><span class="co">#&gt;   0.8169192  0.6946520       0.6640152        </span></span>
<span><span class="co">#&gt;   0.8181818  0.6957021       0.6590909        </span></span>
<span><span class="co">#&gt;   0.8318182  0.7074237       0.6797980        </span></span>
<span><span class="co">#&gt;   0.8181818  0.6955433       0.6424242        </span></span>
<span><span class="co">#&gt;   0.8202020  0.6976679       0.6851010        </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Accuracy was used to select the optimal model using the largest value.</span></span>
<span><span class="co">#&gt; The final value used for the model was mtry = 5.</span></span>
<span><span class="co">#&gt; Warning in nominalTrainWorkflow(x = x, y = y, wts = weights, info = trainInfo,</span></span>
<span><span class="co">#&gt; : There were missing values in resampled performance measures.</span></span>
<span><span class="co">#&gt; Random Forest </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; 140 samples</span></span>
<span><span class="co">#&gt;   7 predictor</span></span>
<span><span class="co">#&gt;   2 classes: 'Cl_1', 'Cl_2' </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Pre-processing: scaled (7), centered (7) </span></span>
<span><span class="co">#&gt; Resampling: Cross-Validated (10 fold, repeated 3 times) </span></span>
<span><span class="co">#&gt; Summary of sample sizes: 127, 126, 126, 126, 126, 126, ... </span></span>
<span><span class="co">#&gt; Addtional sampling using up-sampling prior to pre-processing</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Resampling results across tuning parameters:</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;   mtry  logLoss   AUC        prAUC      Accuracy   Kappa      F1       </span></span>
<span><span class="co">#&gt;   3     1.258600  0.7593434  0.4402517  0.7550183  0.2557136  0.8423889</span></span>
<span><span class="co">#&gt;   4     1.308793  0.7519571  0.4354836  0.7643590  0.2592873  0.8495114</span></span>
<span><span class="co">#&gt;   5     1.234428  0.7224747  0.4337686  0.7580830  0.2334096  0.8463531</span></span>
<span><span class="co">#&gt;   6     1.729796  0.7323232  0.4248192  0.7696703  0.2990418  0.8518427</span></span>
<span><span class="co">#&gt;   7     1.497285  0.7172559  0.4117107  0.7575580  0.2290320  0.8451502</span></span>
<span><span class="co">#&gt;   Sensitivity  Specificity  Pos_Pred_Value  Neg_Pred_Value  Precision</span></span>
<span><span class="co">#&gt;   0.7926768    0.5500000    0.9112595       0.3342687       0.9112595</span></span>
<span><span class="co">#&gt;   0.8063131    0.5333333    0.9096996       0.3219444       0.9096996</span></span>
<span><span class="co">#&gt;   0.8042929    0.5000000    0.9030733       0.3106732       0.9030733</span></span>
<span><span class="co">#&gt;   0.8070707    0.5666667    0.9149893       0.3871100       0.9149893</span></span>
<span><span class="co">#&gt;   0.8037879    0.5000000    0.9043756       0.3077586       0.9043756</span></span>
<span><span class="co">#&gt;   Recall     Detection_Rate  Balanced_Accuracy</span></span>
<span><span class="co">#&gt;   0.7926768  0.6738095       0.6713384        </span></span>
<span><span class="co">#&gt;   0.8063131  0.6855311       0.6698232        </span></span>
<span><span class="co">#&gt;   0.8042929  0.6840171       0.6521465        </span></span>
<span><span class="co">#&gt;   0.8070707  0.6860806       0.6868687        </span></span>
<span><span class="co">#&gt;   0.8037879  0.6834921       0.6518939        </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Accuracy was used to select the optimal model using the largest value.</span></span>
<span><span class="co">#&gt; The final value used for the model was mtry = 6.</span></span>
<span><span class="co">#&gt; Setting levels: control = Cl_1, case = Cl_2</span></span>
<span><span class="co">#&gt; Setting direction: controls &lt; cases</span></span></code></pre></div>
<div class="sourceCode" id="cb18"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># same outputs of binary classification task </span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/names.html" class="external-link">names</a></span><span class="op">(</span><span class="va">rf.RNO</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] "Tuning"                   "Training"                </span></span>
<span><span class="co">#&gt; [3] "Model quality"            "Variableimportance"      </span></span>
<span><span class="co">#&gt; [5] "ROC_Plot"                 "Class Probabilities"     </span></span>
<span><span class="co">#&gt; [7] "Class Probabilities Plot" "Training Data"           </span></span>
<span><span class="co">#&gt; [9] "Test Data"</span></span></code></pre></div>
</div>
<div class="section level3">
<h3 id="regression">Regression<a class="anchor" aria-label="anchor" href="#regression"></a>
</h3>
<p><code>DurumWheatDHEWC</code> dataset is designed for modeling climate
impacts on the days to heading (DHE) of durum wheat. It contains
multiple columns representing climate variables, which are used as
predictors, and a numeric response variable, DHE, which indicates the
number of days required for durum wheat to reach heading under varying
environmental conditions.</p>
<p>This dataset is critical for building and validating regression
models to predict days to heading of durum wheat under specific climatic
scenarios, aiding in understanding and optimizing wheat production
timelines &amp; locations.</p>
<div class="sourceCode" id="cb19"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span></span>
<span><span class="co"># Load sample data for regression task</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/utils/data.html" class="external-link">data</a></span><span class="op">(</span><span class="st">"DurumWheatDHEWC"</span><span class="op">)</span></span>
<span></span>
<span><span class="va">DurumWheatDHEWC</span> <span class="op">&lt;-</span> <span class="fu">icardaFIGSr</span><span class="fu">::</span><span class="va"><a href="../reference/DurumWheatDHEWC.html">DurumWheatDHEWC</a></span></span>
<span></span>
<span> <span class="co"># lets sample DurumWheatDHEWC dataset </span></span>
<span><span class="va">random_cols</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/sample.html" class="external-link">sample</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/colnames.html" class="external-link">colnames</a></span><span class="op">(</span><span class="va">DurumWheatDHEWC</span><span class="op">)</span>, <span class="fl">7</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Note : we can still use the whole dataset for model training and testing. </span></span>
<span>   </span>
<span><span class="co"># Select Y and predictors.</span></span>
<span><span class="va">DurumWheatDHEWC_sample</span> <span class="op">&lt;-</span> <span class="va">DurumWheatDHEWC</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html" class="external-link">%&gt;%</a></span></span>
<span>   <span class="fu">dplyr</span><span class="fu">::</span><span class="fu"><a href="https://dplyr.tidyverse.org/reference/select.html" class="external-link">select</a></span><span class="op">(</span><span class="va">DHE</span>, <span class="fu"><a href="https://tidyselect.r-lib.org/reference/all_of.html" class="external-link">all_of</a></span><span class="op">(</span><span class="va">random_cols</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Check results</span></span>
<span><span class="va">DurumWheatDHEWC_sample</span></span>
<span><span class="co">#&gt;     DHE  tmax_08   tmin_04 prec_01    bio_5  tmax_10     bio_4   tmin_03</span></span>
<span><span class="co">#&gt; 1   172 29.99600  3.064000      73 29.99600 18.54800 1031.8682 -3.128000</span></span>
<span><span class="co">#&gt; 2   178 29.83200  5.072000      43 30.04800 20.42400  777.1078  0.976000</span></span>
<span><span class="co">#&gt; 3   172 27.52000  4.368000      62 27.76800 18.00400  722.3430  0.064000</span></span>
<span><span class="co">#&gt; 4   172 27.60625  7.293750      78 27.75625 19.18125  688.2203  3.193750</span></span>
<span><span class="co">#&gt; 5   178 36.26800  7.300000      89 36.58800 23.40800  994.3781  2.392000</span></span>
<span><span class="co">#&gt; 6   178 30.22000  4.516000     109 30.28800 19.70800  785.1500  0.396000</span></span>
<span><span class="co">#&gt; 7   178 28.58800  4.224000      45 28.64800 18.85200  733.6318  0.048000</span></span>
<span><span class="co">#&gt; 8   176 39.02000  9.368000      75 39.66800 27.30000  964.2764  4.772000</span></span>
<span><span class="co">#&gt; 9   172 31.74000  8.512000     115 32.14400 22.23200  722.0439  4.496000</span></span>
<span><span class="co">#&gt; 10  186 27.29200  6.960000      94 27.53600 19.17200  672.0435  3.240000</span></span>
<span><span class="co">#&gt; 11  178 28.18000  3.988000      86 28.41600 17.86400  745.9142  0.064000</span></span>
<span><span class="co">#&gt; 12  169 33.20000  8.720000     100 33.34400 22.72000  748.7977  4.932000</span></span>
<span><span class="co">#&gt; 13  178 27.83200  4.036000      51 28.04400 18.07600  791.1317 -0.340000</span></span>
<span><span class="co">#&gt; 14  186 29.11600  7.188000      68 29.34800 20.05600  722.8493  3.304000</span></span>
<span><span class="co">#&gt; 15  178 28.69600  5.904000      55 28.70400 19.34800  724.1248  1.808000</span></span>
<span><span class="co">#&gt; 16  178 28.99200  7.308000      69 29.31600 20.15200  716.4745  3.492000</span></span>
<span><span class="co">#&gt; 17  178 25.92800  0.560000      36 25.92800 14.84400  912.1606 -5.552000</span></span>
<span><span class="co">#&gt; 18  178 29.07600  0.428000      34 29.07600 16.73600 1096.3679 -7.268000</span></span>
<span><span class="co">#&gt; 19  167 31.34000  8.392000     124 31.92400 21.61600  728.8510  4.472000</span></span>
<span><span class="co">#&gt; 20  178 33.06800  8.160000     112 33.41200 23.86400  741.2338  4.532000</span></span>
<span><span class="co">#&gt; 21  178 31.83600  8.140000     153 31.83600 22.96400  680.5397  4.916000</span></span>
<span><span class="co">#&gt; 22  178 30.94400  7.148000      70 31.26400 20.52400  743.2798  2.972000</span></span>
<span><span class="co">#&gt; 23  172 29.07600  7.148000     102 29.40800 20.18800  689.7987  3.296000</span></span>
<span><span class="co">#&gt; 24  172 33.16000 11.608000     116 33.16000 28.20000  687.9225  7.884000</span></span>
<span><span class="co">#&gt; 25  172 33.64000 12.348000     108 33.64000 29.04800  683.9843  8.748000</span></span>
<span><span class="co">#&gt; 26  178 31.32400  5.808000      46 31.47600 21.13200  812.9172  1.316000</span></span>
<span><span class="co">#&gt; 27  178 34.30400  6.448000      90 34.56800 21.40800  993.4868  1.128000</span></span>
<span><span class="co">#&gt; 28  178 33.14800  5.820000      82 33.44400 20.12000 1001.4965  0.456000</span></span>
<span><span class="co">#&gt; 29  178 27.73600  4.552000      66 27.93600 18.19600  720.1050  0.344000</span></span>
<span><span class="co">#&gt; 30  178 27.30800  7.424000      98 27.52800 19.52000  630.0882  3.820000</span></span>
<span><span class="co">#&gt; 31  178 28.20000  6.852000      73 28.36400 19.46000  702.7057  2.904000</span></span>
<span><span class="co">#&gt; 32  178 27.01200  3.448000      48 27.20800 17.16800  753.2673 -1.164000</span></span>
<span><span class="co">#&gt; 33  178 29.07600  7.148000     102 29.40800 20.18800  689.7987  3.296000</span></span>
<span><span class="co">#&gt; 34  178 28.03200  3.700000      41 28.13200 18.40400  751.0430 -0.900000</span></span>
<span><span class="co">#&gt; 35  178 25.09200  7.136000      62 25.14000 18.46000  605.3165  3.688000</span></span>
<span><span class="co">#&gt; 36  178 22.58800  2.244000      63 22.58800 14.08000  677.4630 -2.056000</span></span>
<span><span class="co">#&gt; 37  188 27.73600  4.552000      66 27.93600 18.19600  720.1050  0.344000</span></span>
<span><span class="co">#&gt; 38  174 30.43600  7.028000      83 30.92400 21.38000  701.4869  3.116000</span></span>
<span><span class="co">#&gt; 39  178 24.44400  7.000000      71 24.44400 18.30400  609.2167  3.496000</span></span>
<span><span class="co">#&gt; 40  178 29.82000  2.392000      21 29.82000 17.47200 1018.4033 -4.928000</span></span>
<span><span class="co">#&gt; 41  182 26.44800  6.392000      57 26.44800 18.90400  670.9153  2.308000</span></span>
<span><span class="co">#&gt; 42  178 25.46800 -1.116000      32 25.46800 12.90400  994.7368 -7.988000</span></span>
<span><span class="co">#&gt; 43  182 28.15600  4.824000      57 28.15600 18.24000  790.9460 -0.332000</span></span>
<span><span class="co">#&gt; 44  178 27.60800  6.460000      83 27.62400 19.28800  680.3213  2.888000</span></span>
<span><span class="co">#&gt; 45  178 27.30800  7.424000      98 27.52800 19.52000  630.0882  3.820000</span></span>
<span><span class="co">#&gt; 46  178 27.29200  6.960000      94 27.53600 19.17200  672.0435  3.240000</span></span>
<span><span class="co">#&gt; 47  169 29.75200  5.288000     187 29.75200 22.71200  709.8919  1.604000</span></span>
<span><span class="co">#&gt; 48  169 33.34400  8.768000      62 33.64800 25.72800  734.6581  5.256000</span></span>
<span><span class="co">#&gt; 49  178 30.68000  3.592000      52 30.68000 20.33600  829.4178 -0.812000</span></span>
<span><span class="co">#&gt; 50  169 31.57600  8.320000     115 32.07600 22.21600  723.6532  4.328000</span></span>
<span><span class="co">#&gt; 51  174 26.77500 11.154167      82 26.77500 23.53333  366.8831  9.733334</span></span>
<span><span class="co">#&gt; 52  178 30.83600  6.572000      97 31.32800 21.47600  730.1081  2.620000</span></span>
<span><span class="co">#&gt; 53  178 27.27778  7.227778     102 27.73889 19.77222  680.0123  3.544445</span></span>
<span><span class="co">#&gt; 54  164 39.28000  7.416000      88 39.68400 25.56800 1028.8179  2.792000</span></span>
<span><span class="co">#&gt; 55  169 32.50000  8.832000     115 32.99600 23.94800  721.1384  4.964000</span></span>
<span><span class="co">#&gt; 56  172 29.92000  7.828000     102 30.34800 20.35200  694.5842  4.044000</span></span>
<span><span class="co">#&gt; 57  167 37.71200  8.348000      60 37.79600 27.45600  891.1040  4.280000</span></span>
<span><span class="co">#&gt; 58  167 35.65200 11.624001      85 35.65200 28.24000  825.0877  6.800000</span></span>
<span><span class="co">#&gt; 59  174 28.19200  3.248000      45 28.19200 17.32400  886.8262 -2.356000</span></span>
<span><span class="co">#&gt; 60  172 38.69200 13.020000      12 39.62800 26.96000  829.3398  9.084000</span></span>
<span><span class="co">#&gt; 61  176 36.51600  8.308000      12 37.18800 24.20000  808.3892  5.216000</span></span>
<span><span class="co">#&gt; 62  172 36.51600  8.308000      12 37.18800 24.20000  808.3892  5.216000</span></span>
<span><span class="co">#&gt; 63  164 34.70000  7.992000      50 35.47600 24.10400  718.6138  5.476000</span></span>
<span><span class="co">#&gt; 64  164 33.22000 11.368000      33 33.22000 26.48800  578.0347  9.160000</span></span>
<span><span class="co">#&gt; 65  169 33.58800 11.692000      62 33.58800 25.83200  591.3364  9.744000</span></span>
<span><span class="co">#&gt; 66  178 36.98800 10.160000      22 37.60800 25.98800  756.7676  7.236000</span></span>
<span><span class="co">#&gt; 67  164 33.51200  8.300000      43 33.51200 26.80000  727.7678  5.096000</span></span>
<span><span class="co">#&gt; 68  164 33.15600  8.988000      47 33.15600 27.52000  710.1389  5.532000</span></span>
<span><span class="co">#&gt; 69  164 33.94000  8.672000      54 33.94000 27.18800  715.4806  5.696000</span></span>
<span><span class="co">#&gt; 70  164 34.71600  8.124001      59 34.72400 27.21200  730.0200  5.156000</span></span>
<span><span class="co">#&gt; 71  164 34.42400  7.836000      75 34.48000 26.87600  727.0957  4.868000</span></span>
<span><span class="co">#&gt; 72  164 33.27200  7.904000     120 33.27200 26.26800  708.6336  4.904000</span></span>
<span><span class="co">#&gt; 73  167 33.40400  7.116000      38 33.73200 25.24000  763.3324  3.140000</span></span>
<span><span class="co">#&gt; 74  167 33.08800  8.616000      52 33.43200 25.51600  737.6866  4.920000</span></span>
<span><span class="co">#&gt; 75  167 34.20800  8.024000      82 34.30400 26.68800  724.0570  5.008000</span></span>
<span><span class="co">#&gt; 76  167 39.80800  9.592000      60 40.46000 28.31600  947.7985  4.808000</span></span>
<span><span class="co">#&gt; 77  167 39.62400  9.356000      68 40.23200 28.10400  944.2841  4.704000</span></span>
<span><span class="co">#&gt; 78  167 40.42400  9.680000      71 41.02000 28.47200  959.8204  5.108000</span></span>
<span><span class="co">#&gt; 79  167 38.61600 10.400000      53 39.22400 27.66400  933.1219  5.260000</span></span>
<span><span class="co">#&gt; 80  167 34.10400 11.544000     125 34.10400 26.48000  810.0171  6.872000</span></span>
<span><span class="co">#&gt; 81  169 33.65200 11.604000     133 33.65200 26.26000  793.5392  7.144000</span></span>
<span><span class="co">#&gt; 82  164 35.81200  9.020000      42 35.81200 27.08000  836.1251  4.772000</span></span>
<span><span class="co">#&gt; 83  164 30.44800 14.020000     167 30.44800 27.98800  545.1089 10.492000</span></span>
<span><span class="co">#&gt; 84  167 31.88800  8.276000     153 31.88800 23.49200  664.2324  5.116000</span></span>
<span><span class="co">#&gt; 85  172 36.69200  7.344000      96 37.10800 23.23200 1015.4501  2.264000</span></span>
<span><span class="co">#&gt; 86  176 34.44800  6.280000      92 34.48000 23.21600  905.4482  1.788000</span></span>
<span><span class="co">#&gt; 87  174 37.77600  7.044000     103 38.11200 24.22000 1021.2753  2.136000</span></span>
<span><span class="co">#&gt; 88  174 37.90400  9.504000      61 38.79200 27.63200  940.4232  5.116000</span></span>
<span><span class="co">#&gt; 89  174 30.65200 12.856000     166 30.65200 25.94400  670.3594  9.168000</span></span>
<span><span class="co">#&gt; 90  172 35.94400  7.124000     106 36.26400 22.52000 1016.0784  1.940000</span></span>
<span><span class="co">#&gt; 91  178 26.36400  5.996000      58 26.46000 17.62000  687.8534  1.956000</span></span>
<span><span class="co">#&gt; 92  178 25.54400  7.016000     113 25.61200 18.82000  592.0856  3.280000</span></span>
<span><span class="co">#&gt; 93  178 26.37200  5.652000      58 26.42000 17.68000  687.7638  1.668000</span></span>
<span><span class="co">#&gt; 94  178 25.54400  7.016000     113 25.61200 18.82000  592.0856  3.280000</span></span>
<span><span class="co">#&gt; 95  178 38.47600  7.056000      93 39.14400 24.66400 1036.8070  2.460000</span></span>
<span><span class="co">#&gt; 96  174 36.89200  8.132000      91 37.08800 25.63600  916.9009  3.740000</span></span>
<span><span class="co">#&gt; 97  178 26.55600  3.068000      46 26.70000 16.11200  780.1105 -1.588000</span></span>
<span><span class="co">#&gt; 98  172 34.21600  8.160000      38 34.21600 26.02000  794.1812  3.912000</span></span>
<span><span class="co">#&gt; 99  174 30.73200 12.552000     166 30.73200 26.46400  648.8869  8.804000</span></span>
<span><span class="co">#&gt; 100 167 37.10400 11.304000      21 37.47200 28.42800  863.9432  6.320000</span></span>
<span><span class="co">#&gt; 101 167 35.75600 10.508000      46 35.75600 27.36400  617.1011  8.372000</span></span>
<span><span class="co">#&gt; 102 178 30.67200  5.076000      33 31.17200 20.02000  703.4542  2.040000</span></span>
<span><span class="co">#&gt; 103 164 36.03600 10.008000      15 37.15600 25.35200  746.0551  6.892000</span></span>
<span><span class="co">#&gt; 104 169 31.60000  8.108000      60 31.83200 20.86000  614.7405  5.028000</span></span>
<span><span class="co">#&gt; 105 178 31.18400 11.644000      79 31.18400 23.92400  468.2367  9.756000</span></span>
<span><span class="co">#&gt; 106 172 38.82800 13.924000      15 39.62000 27.33200  806.1729 10.700000</span></span>
<span><span class="co">#&gt; 107 172 31.80400  6.380000     118 31.80400 21.72400  663.1384  4.140000</span></span>
<span><span class="co">#&gt; 108 172 32.36400  6.464000     104 32.36400 21.82400  680.3736  4.180000</span></span>
<span><span class="co">#&gt; 109 164 32.59200  7.032000      81 32.85600 22.33600  680.1926  4.508000</span></span>
<span><span class="co">#&gt; 110 172 33.50800  7.532000      30 34.65200 22.33600  734.3494  4.472000</span></span>
<span><span class="co">#&gt; 111 174 30.49600  7.500000      23 31.39200 20.71600  674.7256  4.392000</span></span>
<span><span class="co">#&gt; 112 169 34.03200  8.236000      44 34.72000 22.52400  770.6617  5.272000</span></span>
<span><span class="co">#&gt; 113 174 33.32800 10.132000     159 33.32800 24.99600  624.0789  8.076000</span></span>
<span><span class="co">#&gt; 114 167 33.90800 10.008000     140 33.90800 25.70800  628.1129  8.000000</span></span>
<span><span class="co">#&gt; 115 172 31.58800  9.612000     109 31.58800 24.75200  565.8553  7.756000</span></span>
<span><span class="co">#&gt; 116 174 37.93600  7.396000      96 38.38800 24.98400  995.0492  2.896000</span></span>
<span><span class="co">#&gt; 117 174 30.58800  5.548000      44 30.64400 20.74800  795.6289  1.088000</span></span>
<span><span class="co">#&gt; 118 178 30.26800  4.652000      57 30.45200 20.10400  782.5659  0.388000</span></span>
<span><span class="co">#&gt; 119 178 29.16400  3.132000      43 29.16400 18.65200  779.7481 -1.236000</span></span>
<span><span class="co">#&gt; 120 178 39.92000 10.668000      75 40.56000 28.13200  961.6358  6.316000</span></span>
<span><span class="co">#&gt; 121 172 33.74400  7.532000     107 33.74400 23.57600  849.7675  3.124000</span></span>
<span><span class="co">#&gt; 122 178 31.35263 12.936842     115 31.35263 26.71579  634.2849  9.036842</span></span>
<span><span class="co">#&gt; 123 172 31.74000  8.512000     115 32.14400 22.23200  722.0439  4.496000</span></span>
<span><span class="co">#&gt; 124 167 35.86800 11.004000     115 35.86800 28.46800  737.8636  7.152000</span></span>
<span><span class="co">#&gt; 125 174 29.78800  4.052000      78 29.91200 19.75600  752.1466  0.280000</span></span>
<span><span class="co">#&gt; 126 169 34.50000  9.520000     122 35.17200 25.19200  724.5495  5.984000</span></span>
<span><span class="co">#&gt; 127 169 32.72400 10.176000     177 32.82800 24.70000  646.5800  7.196000</span></span>
<span><span class="co">#&gt; 128 169 34.08800  8.208000     103 34.36800 23.69600  740.8456  4.140000</span></span>
<span><span class="co">#&gt; 129 178 31.00800  5.700000      45 31.16800 21.11600  803.4934  1.224000</span></span>
<span><span class="co">#&gt; 130 178 26.85600  2.812000      60 26.85600 16.99600  772.0298 -1.672000</span></span>
<span><span class="co">#&gt; 131 178 28.90800  3.752000      57 29.01600 18.72800  786.7649 -0.644000</span></span>
<span><span class="co">#&gt; 132 178 30.89600  4.948000      48 30.89600 20.41600  822.2949  0.332000</span></span>
<span><span class="co">#&gt; 133 182 33.82800  6.152000      87 34.05200 20.87200 1003.6749  0.760000</span></span>
<span><span class="co">#&gt; 134 176 38.80800  8.412000      93 39.10000 25.91600  995.3826  3.676000</span></span>
<span><span class="co">#&gt; 135 169 38.66800  9.936000      73 39.38400 27.00800  962.7939  5.292000</span></span>
<span><span class="co">#&gt; 136 172 28.40400  6.560000      99 28.96000 20.22800  694.9699  2.800000</span></span>
<span><span class="co">#&gt; 137 167 34.68800  4.928000     109 34.68800 21.21600 1030.0599 -0.504000</span></span>
<span><span class="co">#&gt; 138 167 33.34400  8.768000      62 33.64800 25.72800  734.6581  5.256000</span></span>
<span><span class="co">#&gt; 139 178 28.96400  9.748000      71 28.96400 24.58800  428.4697  8.380000</span></span>
<span><span class="co">#&gt; 140 176 28.96400  9.748000      71 28.96400 24.58800  428.4697  8.380000</span></span>
<span><span class="co">#&gt; 141 178 32.46000  8.744000      46 32.74000 23.21200  631.4650  6.204000</span></span>
<span><span class="co">#&gt; 142 172 33.44400  7.504000      52 33.74400 23.34000  679.1700  4.712000</span></span>
<span><span class="co">#&gt; 143 178 32.73200  9.300000      92 32.73200 27.50800  661.4465  6.412000</span></span>
<span><span class="co">#&gt; 144 172 32.44400 10.312000      84 32.44400 27.62400  641.1365  7.176000</span></span>
<span><span class="co">#&gt; 145 172 40.37600  9.472000      75 40.95200 28.24000  962.0396  5.012000</span></span>
<span><span class="co">#&gt; 146 172 34.78000 10.844000      93 34.78000 27.15200  829.7251  6.324000</span></span>
<span><span class="co">#&gt; 147 172 34.67200  8.556000      40 34.67200 26.52800  804.2781  4.272000</span></span>
<span><span class="co">#&gt; 148 172 29.86400  8.108000     185 29.86400 22.72400  738.3915  3.488000</span></span>
<span><span class="co">#&gt; 149 172 33.18400 12.592000     144 33.18400 27.13200  731.0255  8.372000</span></span>
<span><span class="co">#&gt; 150 176 39.08000 10.600000      32 39.50400 29.62800  922.9731  5.800000</span></span>
<span><span class="co">#&gt; 151 178 33.16400  7.816000      59 33.16400 27.01200  716.3182  4.608000</span></span>
<span><span class="co">#&gt; 152 178 33.12000  8.868000      87 33.12000 27.39200  672.4055  6.136000</span></span>
<span><span class="co">#&gt; 153 172 34.20800 12.660000     118 34.20800 28.67600  720.9083  8.548000</span></span>
<span><span class="co">#&gt; 154 167 41.34400 11.104000      85 42.02000 29.48000  969.0424  7.328000</span></span>
<span><span class="co">#&gt; 155 172 29.18000  7.576000     199 29.18000 22.43600  717.5812  3.000000</span></span>
<span><span class="co">#&gt; 156 164 33.74000 11.788000      54 33.74000 26.11600  588.4982  9.835999</span></span>
<span><span class="co">#&gt; 157 164 32.00000  6.660000     111 32.32400 21.93200  675.8183  4.320000</span></span>
<span><span class="co">#&gt; 158 164 32.00000  6.660000     111 32.32400 21.93200  675.8183  4.320000</span></span>
<span><span class="co">#&gt; 159 167 32.29200  5.904000      57 32.29200 25.86400  733.6518  2.484000</span></span>
<span><span class="co">#&gt; 160 169 32.29200  5.904000      57 32.29200 25.86400  733.6518  2.484000</span></span>
<span><span class="co">#&gt; 161 172 27.56400  2.592000     147 27.56400 20.22800  693.3407 -1.268000</span></span>
<span><span class="co">#&gt; 162 164 39.76800 12.048000      39 40.44800 29.18000  961.7704  6.780000</span></span>
<span><span class="co">#&gt; 163 167 35.27200 11.388000     115 35.27200 28.65200  772.2361  7.116000</span></span>
<span><span class="co">#&gt; 164 178 34.73200 11.188000      68 34.73200 28.97600  644.2667  7.892000</span></span>
<span><span class="co">#&gt; 165 174 35.22000  9.076000      69 35.52800 26.11600  662.0358  6.696000</span></span>
<span><span class="co">#&gt; 166 178 33.96800 11.520000      33 34.46800 26.08800  576.8907  9.724000</span></span>
<span><span class="co">#&gt; 167 178 33.34400  8.560000      55 33.66400 25.70800  734.9302  4.964000</span></span>
<span><span class="co">#&gt; 168 172 30.69286 13.507143     153 30.69286 27.43571  574.4055 10.428572</span></span>
<span><span class="co">#&gt; 169 172 26.68400  3.316000      43 27.05200 16.74000  752.8312 -1.276000</span></span>
<span><span class="co">#&gt; 170 178 30.14000  6.116000      55 30.42800 19.72400  808.4551  1.516000</span></span>
<span><span class="co">#&gt; 171 178 28.69600  3.940000      47 28.72000 18.16800  831.3232 -1.076000</span></span>
<span><span class="co">#&gt; 172 178 25.34800  4.212000      48 25.34800 17.28800  694.9345 -0.044000</span></span>
<span><span class="co">#&gt; 173 178 28.08400  2.100000      39 28.08400 17.92400  815.6906 -2.556000</span></span>
<span><span class="co">#&gt; 174 172 33.62000  9.132000      53 33.62000 28.00000  683.8333  5.932000</span></span>
<span><span class="co">#&gt; 175 169 34.40400  8.464000      60 34.59600 24.79600  669.4826  6.380000</span></span>
<span><span class="co">#&gt; 176 182 30.07600  8.440001      57 30.20400 20.60400  564.0870  5.740000</span></span>
<span><span class="co">#&gt; 177 172 30.03200 12.440001     113 30.03200 23.46000  457.2107 10.924000</span></span>
<span><span class="co">#&gt; 178 167 31.09600 12.536000     158 31.09600 27.02000  635.5856  8.704000</span></span>
<span><span class="co">#&gt; 179 172 35.40400  6.992000      97 35.42400 24.22800  923.7162  2.540000</span></span>
<span><span class="co">#&gt; 180 188 30.94400  7.148000      70 31.26400 20.52400  743.2798  2.972000</span></span>
<span><span class="co">#&gt; 181 182 29.32400  4.664000      44 29.76000 18.92400  837.3220 -0.304000</span></span>
<span><span class="co">#&gt; 182 178 27.39200  6.544000      77 27.54400 19.01600  686.9943  2.640000</span></span>
<span><span class="co">#&gt; 183 184 35.63200  8.188000      78 35.94000 22.97600  986.1057  3.308000</span></span>
<span><span class="co">#&gt; 184 178 32.62800  9.424000     123 33.20000 23.16000  724.1036  5.536000</span></span>
<span><span class="co">#&gt; 185 178 39.28000 12.860000      24 40.29600 29.12000  947.2092  7.272000</span></span>
<span><span class="co">#&gt; 186 172 40.38000 10.016000      63 41.04000 28.73600  959.3826  5.232000</span></span>
<span><span class="co">#&gt; 187 172 36.21200  9.968000      61 36.21200 27.99600  829.4252  5.616000</span></span>
<span><span class="co">#&gt; 188 167 38.33200 11.320000      21 38.72800 28.48000  902.8373  6.228000</span></span>
<span><span class="co">#&gt; 189 174 30.50000 10.548000     178 30.50000 25.04800  685.8725  6.232000</span></span>
<span><span class="co">#&gt; 190 169 39.02000  9.368000      75 39.66800 27.30000  964.2764  4.772000</span></span>
<span><span class="co">#&gt; 191 178 39.68400  8.732000      70 40.22800 27.98800  947.0474  4.340000</span></span>
<span><span class="co">#&gt; 192 167 35.43200  9.380000      32 36.54400 24.92000  720.2234  6.800000</span></span>
<span><span class="co">#&gt; 193 167 33.73200 10.580000      95 33.73200 27.12800  758.0823  6.364000</span></span></code></pre></div>
<div class="sourceCode" id="cb20"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co">## Regression of DHE (days to heading)</span></span>
<span><span class="va">svm.DHE</span> <span class="op">&lt;-</span> <span class="fu">icardaFIGSr</span><span class="fu">::</span><span class="fu"><a href="../reference/tuneTrain.html">tuneTrain</a></span><span class="op">(</span>data <span class="op">=</span> <span class="va">DurumWheatDHEWC_sample</span>,</span>
<span>                      y <span class="op">=</span>  <span class="st">'DHE'</span>,</span>
<span>                      method <span class="op">=</span> <span class="st">'svmLinear2'</span>,</span>
<span>                      summary <span class="op">=</span> <span class="va">defaultSummary</span>,</span>
<span>                      classProbs <span class="op">=</span> <span class="cn">FALSE</span>,</span>
<span>                      repeats <span class="op">=</span> <span class="fl">3</span><span class="op">)</span></span>
<span><span class="co">#&gt; Support Vector Machines with Linear Kernel </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; 137 samples</span></span>
<span><span class="co">#&gt;   7 predictor</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Pre-processing: centered (7), scaled (7) </span></span>
<span><span class="co">#&gt; Resampling: Cross-Validated (10 fold, repeated 3 times) </span></span>
<span><span class="co">#&gt; Summary of sample sizes: 124, 123, 123, 123, 124, 123, ... </span></span>
<span><span class="co">#&gt; Resampling results across tuning parameters:</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;   cost    RMSE      Rsquared   MAE     </span></span>
<span><span class="co">#&gt;     0.25  4.475290  0.3332189  3.639160</span></span>
<span><span class="co">#&gt;     0.50  4.478288  0.3321966  3.634820</span></span>
<span><span class="co">#&gt;     1.00  4.483408  0.3316731  3.633663</span></span>
<span><span class="co">#&gt;     2.00  4.492265  0.3294722  3.635448</span></span>
<span><span class="co">#&gt;     4.00  4.504773  0.3266474  3.639368</span></span>
<span><span class="co">#&gt;     8.00  4.519448  0.3232051  3.651156</span></span>
<span><span class="co">#&gt;    16.00  4.537546  0.3205538  3.664934</span></span>
<span><span class="co">#&gt;    32.00  4.543125  0.3193189  3.669879</span></span>
<span><span class="co">#&gt;    64.00  4.555453  0.3167104  3.677691</span></span>
<span><span class="co">#&gt;   128.00  4.560078  0.3152932  3.681401</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; RMSE was used to select the optimal model using the smallest value.</span></span>
<span><span class="co">#&gt; The final value used for the model was cost = 0.25.</span></span>
<span><span class="co">#&gt; Support Vector Machines with Linear Kernel </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; 137 samples</span></span>
<span><span class="co">#&gt;   7 predictor</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Pre-processing: centered (7), scaled (7) </span></span>
<span><span class="co">#&gt; Resampling: Cross-Validated (10 fold, repeated 3 times) </span></span>
<span><span class="co">#&gt; Summary of sample sizes: 123, 123, 124, 123, 123, 124, ... </span></span>
<span><span class="co">#&gt; Resampling results across tuning parameters:</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;   cost  RMSE      Rsquared   MAE     </span></span>
<span><span class="co">#&gt;   0.25  4.441064  0.3451608  3.609214</span></span>
<span><span class="co">#&gt;   0.50  4.436608  0.3483457  3.596602</span></span>
<span><span class="co">#&gt;   0.75  4.450294  0.3473019  3.599619</span></span>
<span><span class="co">#&gt;   1.00  4.461562  0.3445030  3.607569</span></span>
<span><span class="co">#&gt;   1.25  4.463952  0.3441880  3.610179</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; RMSE was used to select the optimal model using the smallest value.</span></span>
<span><span class="co">#&gt; The final value used for the model was cost = 0.5.</span></span></code></pre></div>
<div class="sourceCode" id="cb21"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">svm.DHE</span><span class="op">$</span><span class="va">VariableImportance</span></span></code></pre></div>
<div class="figure">
<img src="ML_Workflows_files/figure-html/unnamed-chunk-21-1.png" alt="Variable importance DHE regression" width="672"><p class="caption">
Variable importance DHE regression
</p>
</div>
<div class="sourceCode" id="cb22"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">svm.DHE</span><span class="op">$</span><span class="va">Quality_metrics</span></span>
<span><span class="co">#&gt;      RMSE  Rsquared       MAE </span></span>
<span><span class="co">#&gt; 4.4294787 0.3721135 3.5262731</span></span>
<span><span class="va">svm.DHE</span><span class="op">$</span><span class="va">`Predicted vs Actual Plot`</span></span>
<span><span class="co">#&gt; `geom_smooth()` using formula = 'y ~ x'</span></span></code></pre></div>
<div class="figure">
<img src="ML_Workflows_files/figure-html/unnamed-chunk-22-1.png" alt="Predicted vs Actual DHE regression" width="672"><p class="caption">
Predicted vs Actual DHE regression
</p>
</div>
<div class="sourceCode" id="cb23"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">svm.DHE</span><span class="op">$</span><span class="va">`Residuals Vs. Predicted Plot`</span></span></code></pre></div>
<div class="figure">
<img src="ML_Workflows_files/figure-html/unnamed-chunk-23-1.png" alt="Residuals vs predicted DHE regression" width="672"><p class="caption">
Residuals vs predicted DHE regression
</p>
</div>
<div class="sourceCode" id="cb24"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">svm.DHE</span><span class="op">$</span><span class="va">Training</span></span>
<span><span class="co">#&gt; Support Vector Machines with Linear Kernel </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; 137 samples</span></span>
<span><span class="co">#&gt;   7 predictor</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Pre-processing: centered (7), scaled (7) </span></span>
<span><span class="co">#&gt; Resampling: Cross-Validated (10 fold, repeated 3 times) </span></span>
<span><span class="co">#&gt; Summary of sample sizes: 123, 123, 124, 123, 123, 124, ... </span></span>
<span><span class="co">#&gt; Resampling results across tuning parameters:</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;   cost  RMSE      Rsquared   MAE     </span></span>
<span><span class="co">#&gt;   0.25  4.441064  0.3451608  3.609214</span></span>
<span><span class="co">#&gt;   0.50  4.436608  0.3483457  3.596602</span></span>
<span><span class="co">#&gt;   0.75  4.450294  0.3473019  3.599619</span></span>
<span><span class="co">#&gt;   1.00  4.461562  0.3445030  3.607569</span></span>
<span><span class="co">#&gt;   1.25  4.463952  0.3441880  3.610179</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; RMSE was used to select the optimal model using the smallest value.</span></span>
<span><span class="co">#&gt; The final value used for the model was cost = 0.5.</span></span></code></pre></div>
<div class="sourceCode" id="cb25"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">svm.DHE</span><span class="op">$</span><span class="va">Tuning</span> </span>
<span><span class="co">#&gt; Support Vector Machines with Linear Kernel </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; 137 samples</span></span>
<span><span class="co">#&gt;   7 predictor</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Pre-processing: centered (7), scaled (7) </span></span>
<span><span class="co">#&gt; Resampling: Cross-Validated (10 fold, repeated 3 times) </span></span>
<span><span class="co">#&gt; Summary of sample sizes: 124, 123, 123, 123, 124, 123, ... </span></span>
<span><span class="co">#&gt; Resampling results across tuning parameters:</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;   cost    RMSE      Rsquared   MAE     </span></span>
<span><span class="co">#&gt;     0.25  4.475290  0.3332189  3.639160</span></span>
<span><span class="co">#&gt;     0.50  4.478288  0.3321966  3.634820</span></span>
<span><span class="co">#&gt;     1.00  4.483408  0.3316731  3.633663</span></span>
<span><span class="co">#&gt;     2.00  4.492265  0.3294722  3.635448</span></span>
<span><span class="co">#&gt;     4.00  4.504773  0.3266474  3.639368</span></span>
<span><span class="co">#&gt;     8.00  4.519448  0.3232051  3.651156</span></span>
<span><span class="co">#&gt;    16.00  4.537546  0.3205538  3.664934</span></span>
<span><span class="co">#&gt;    32.00  4.543125  0.3193189  3.669879</span></span>
<span><span class="co">#&gt;    64.00  4.555453  0.3167104  3.677691</span></span>
<span><span class="co">#&gt;   128.00  4.560078  0.3152932  3.681401</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; RMSE was used to select the optimal model using the smallest value.</span></span>
<span><span class="co">#&gt; The final value used for the model was cost = 0.25.</span></span></code></pre></div>
</div>
<div class="section level3">
<h3 id="multiclass-classification">Multiclass classification<a class="anchor" aria-label="anchor" href="#multiclass-classification"></a>
</h3>
<p>In this case, we use the same <code>DurumWheatDHEWC</code> dataset to
create days to heading classes variable (DHE_Class) to fit a multiclass
model. DHE_classes are descibed as follow : <br><strong>1
(Early):</strong> Represents samples/observations with early days to
heading, indicating adaptability to shorter growing seasons or favorable
early-season conditions. <br><strong>2 (Intermediate):</strong>
Represents samples/observations with moderate days to heading,
indicating typical or average responses under given environmental
conditions. <br><strong>3 (Late):</strong> Represents
samples/observations with late days to heading, suggesting adaptability
to longer growing seasons or late-season conditions. <br></p>
<p>These levels can be more than three and have different ranges.</p>
<div class="sourceCode" id="cb26"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span></span>
<span><span class="co">## Multiclass classification of DHE Classes with imbalanced data </span></span>
<span></span>
<span><span class="co"># Create DHE Classes from DurumWheatDHEWC dataset </span></span>
<span><span class="va">DurumWheatDHEWC_sample</span><span class="op">$</span><span class="va">DHE_class</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/ifelse.html" class="external-link">ifelse</a></span><span class="op">(</span></span>
<span>   <span class="va">DurumWheatDHEWC_sample</span><span class="op">$</span><span class="va">DHE</span> <span class="op">&lt;=</span> <span class="fl">172</span>,<span class="st">"1"</span>,</span>
<span>   <span class="fu"><a href="https://rdrr.io/r/base/ifelse.html" class="external-link">ifelse</a></span><span class="op">(</span><span class="va">DurumWheatDHEWC_sample</span><span class="op">$</span><span class="va">DHE</span> <span class="op">&lt;=</span> <span class="fl">180</span>, <span class="st">"2"</span>, <span class="st">"3"</span><span class="op">)</span></span>
<span> <span class="op">)</span></span>
<span> </span>
<span><span class="co"># convert to factor</span></span>
<span> </span>
<span><span class="va">DurumWheatDHEWC_sample</span><span class="op">$</span><span class="va">DHE_class</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/factor.html" class="external-link">factor</a></span><span class="op">(</span><span class="va">DurumWheatDHEWC_sample</span><span class="op">$</span><span class="va">DHE_class</span><span class="op">)</span></span>
<span>   </span>
<span><span class="co"># Count classes for data imbalance check</span></span>
<span><span class="va">DurumWheatDHEWC_sample</span><span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html" class="external-link">%&gt;%</a></span></span>
<span>   <span class="fu"><a href="https://dplyr.tidyverse.org/reference/count.html" class="external-link">count</a></span><span class="op">(</span><span class="va">DHE_class</span><span class="op">)</span></span>
<span><span class="co">#&gt;   DHE_class  n</span></span>
<span><span class="co">#&gt; 1         1 96</span></span>
<span><span class="co">#&gt; 2         2 87</span></span>
<span><span class="co">#&gt; 3         3 10</span></span></code></pre></div>
<div class="sourceCode" id="cb27"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span></span>
<span><span class="co">## Not run:</span></span>
<span></span>
<span><span class="co"># Run Multiclass Classification</span></span>
<span><span class="va">rf.DHE_class</span> <span class="op">&lt;-</span> <span class="fu">icardaFIGSr</span><span class="fu">::</span><span class="fu"><a href="../reference/tuneTrain.html">tuneTrain</a></span><span class="op">(</span>data <span class="op">=</span> <span class="va">DurumWheatDHEWC_sample</span>,</span>
<span>                              y <span class="op">=</span>  <span class="st">'DHE_class'</span>,</span>
<span>                              method <span class="op">=</span> <span class="st">'rf'</span>,</span>
<span>                              parallelComputing <span class="op">=</span> <span class="cn">TRUE</span>,</span>
<span>                              summary <span class="op">=</span> <span class="va">multiClassSummary</span>,</span>
<span>                              imbalanceMethod <span class="op">=</span><span class="st">"up"</span>, <span class="co"># Here we upsample less represented class (3)</span></span>
<span>                              imbalanceThreshold <span class="op">=</span> <span class="fl">0.2</span>,</span>
<span>                              classProbs <span class="op">=</span> <span class="cn">TRUE</span>,</span>
<span>                              repeats <span class="op">=</span> <span class="fl">3</span><span class="op">)</span></span>
<span><span class="va">rf.DHE_class</span></span>
<span></span>
<span><span class="co">## End(Not run)</span></span></code></pre></div>
</div>
</div>
<div class="section level2">
<h2 id="splitdata">splitData()<a class="anchor" aria-label="anchor" href="#splitdata"></a>
</h2>
<div class="sourceCode" id="cb28"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span></span>
<span><span class="co"># Subset septoriaDurumWC where column names having 3, ex tmin3, prec13</span></span>
<span><span class="va">septoriaDurumWC_subset</span> <span class="op">&lt;-</span> <span class="fu">icardaFIGSr</span><span class="fu">::</span><span class="va"><a href="../reference/septoriaDurumWC.html">septoriaDurumWC</a></span><span class="op">|&gt;</span></span>
<span>  <span class="fu">dplyr</span><span class="fu">::</span><span class="fu"><a href="https://dplyr.tidyverse.org/reference/select.html" class="external-link">select</a></span><span class="op">(</span><span class="va">ST_S</span>,<span class="fu"><a href="https://tidyselect.r-lib.org/reference/starts_with.html" class="external-link">contains</a></span><span class="op">(</span><span class="st">"3"</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># split data</span></span>
<span><span class="va">septoriaDurumWC_subset_split</span> <span class="op">&lt;-</span> <span class="fu">icardaFIGSr</span><span class="fu">::</span><span class="fu"><a href="../reference/splitData.html">splitData</a></span><span class="op">(</span><span class="va">septoriaDurumWC</span>,</span>
<span>                        seed <span class="op">=</span> <span class="fl">123</span>, y<span class="op">=</span><span class="st">"ST_S"</span>, p<span class="op">=</span><span class="fl">0.7</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Check results</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/names.html" class="external-link">names</a></span><span class="op">(</span><span class="va">septoriaDurumWC_subset_split</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] "trainset" "testset"</span></span></code></pre></div>
</div>
<div class="section level2">
<h2 id="getmetrics">getMetrics()<a class="anchor" aria-label="anchor" href="#getmetrics"></a>
</h2>
<div class="sourceCode" id="cb29"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span></span>
<span><span class="co"># Call the ST_S knn model fitted in tunTrain function section</span></span>
<span></span>
<span><span class="va">data.test</span> <span class="op">&lt;-</span> <span class="va">knn.ST_S</span><span class="op">$</span><span class="va">`Test Data`</span></span>
<span></span>
<span><span class="va">pred.ST_S</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/predict.html" class="external-link">predict</a></span><span class="op">(</span><span class="va">knn.ST_S</span><span class="op">$</span><span class="va">Tuning</span>, newdata <span class="op">=</span> <span class="va">data.test</span><span class="op">[</span> , <span class="op">-</span><span class="fl">1</span><span class="op">]</span><span class="op">)</span></span>
<span></span>
<span><span class="va">metrics.knn.ST_S</span> <span class="op">&lt;-</span> <span class="fu">icardaFIGSr</span><span class="fu">::</span><span class="fu"><a href="../reference/getMetrics.html">getMetrics</a></span><span class="op">(</span>y <span class="op">=</span> <span class="va">data.test</span><span class="op">$</span><span class="va">ST_S</span>,</span>
<span>                                       yhat <span class="op">=</span> <span class="va">pred.ST_S</span>, classtype <span class="op">=</span> <span class="fl">2</span><span class="op">)</span></span>
<span></span>
<span><span class="va">metrics.knn.ST_S</span></span>
<span><span class="co">#&gt; $Metrics</span></span>
<span><span class="co">#&gt;                            Metrics</span></span>
<span><span class="co">#&gt; Accuracy                     0.729</span></span>
<span><span class="co">#&gt; 95% CI              (0.597, 0.836)</span></span>
<span><span class="co">#&gt; No Information Rate          0.525</span></span>
<span><span class="co">#&gt; P-Value [Acc &gt; NIR]     0.00113022</span></span>
<span><span class="co">#&gt; Kappa                        0.458</span></span>
<span><span class="co">#&gt; Sensitivity                   0.71</span></span>
<span><span class="co">#&gt; Specificity                   0.75</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; $CM</span></span>
<span><span class="co">#&gt;    R  S</span></span>
<span><span class="co">#&gt; R 22  7</span></span>
<span><span class="co">#&gt; S  9 21</span></span></code></pre></div>
</div>
<div class="section level2">
<h2 id="getmetricspca">getMetricsPCA()<a class="anchor" aria-label="anchor" href="#getmetricspca"></a>
</h2>
<p><br> Please run below code chunk to test <code><a href="../reference/getMetricsPCA.html">getMetricsPCA()</a></code>
on any model rutrned by <code><a href="../reference/tuneTrain.html">tuneTrain()</a></code>. <br></p>
<div class="sourceCode" id="cb30"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span></span>
<span><span class="co">## Run Binary classification of ST_S with balanced data using random forest</span></span>
<span><span class="va">rf.ST_S</span> <span class="op">&lt;-</span> <span class="fu">icardaFIGSr</span><span class="fu">::</span><span class="fu"><a href="../reference/tuneTrain.html">tuneTrain</a></span><span class="op">(</span></span>
<span>                      data <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/as.data.frame.html" class="external-link">as.data.frame</a></span><span class="op">(</span><span class="va">septoriaDurumWC_sample</span><span class="op">)</span>,</span>
<span>                      y <span class="op">=</span>  <span class="st">'ST_S'</span>,</span>
<span>                      method <span class="op">=</span> <span class="st">'rf'</span>, <span class="co"># using rf algorithm</span></span>
<span>                      summary <span class="op">=</span> <span class="va">multiClassSummary</span>, <span class="co"># Important for classification tasks  </span></span>
<span>                      parallelComputing <span class="op">=</span> <span class="cn">T</span>,</span>
<span>                      repeats <span class="op">=</span> <span class="fl">3</span>,</span>
<span>                      process <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="st">"center"</span>,<span class="st">"scale"</span><span class="op">)</span>,</span>
<span>                      classProbs <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span> <span class="co"># Important for classification tasks</span></span>
<span></span>
<span><span class="co"># get test data from one of the model to be used for prediction</span></span>
<span><span class="va">data.test</span> <span class="op">&lt;-</span> <span class="va">rf.ST_S</span><span class="op">$</span><span class="va">`Test Data`</span></span>
<span></span>
<span><span class="co"># Obtain predictions from previously run models : knn.ST_S and rf.ST_S</span></span>
<span> <span class="va">pred.knn.ST_S</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/predict.html" class="external-link">predict</a></span><span class="op">(</span><span class="va">knn.ST_S</span><span class="op">$</span><span class="va">Tuning</span>, newdata <span class="op">=</span> <span class="va">data.test</span><span class="op">[</span> , <span class="op">-</span><span class="fl">1</span><span class="op">]</span><span class="op">)</span></span>
<span><span class="va">pred.rf.ST_S</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/predict.html" class="external-link">predict</a></span><span class="op">(</span><span class="va">rf.ST_S</span><span class="op">$</span><span class="va">Tuning</span>, newdata <span class="op">=</span> <span class="va">data.test</span><span class="op">[</span> , <span class="op">-</span><span class="fl">1</span><span class="op">]</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Get metrics for your model using computed metrics.knn.ST_S</span></span>
<span><span class="va">metrics.knn.ST_S</span> <span class="op">&lt;-</span> <span class="va">metrics.knn.ST_S</span></span>
<span><span class="va">metrics.rf.ST_S</span> <span class="op">&lt;-</span> <span class="fu">icardaFIGSr</span><span class="fu">::</span><span class="fu"><a href="../reference/getMetrics.html">getMetrics</a></span><span class="op">(</span>y <span class="op">=</span> <span class="va">data.test</span><span class="op">$</span><span class="va">ST_S</span>,</span>
<span>                         yhat <span class="op">=</span> <span class="va">pred.rf.ST_S</span>, classtype <span class="op">=</span> <span class="fl">2</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Indexing for 2-class models to remove extra column with</span></span>
<span><span class="co"># names of performance measures</span></span>
<span><span class="va">metrics.all</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/cbind.html" class="external-link">cbind</a></span><span class="op">(</span><span class="va">metrics.knn.ST_S</span>, <span class="va">metrics.rf.ST_S</span><span class="op">)</span></span>
<span>  </span>
<span><span class="co">## check data structure</span></span>
<span><span class="va">metrics.all</span></span>
<span>  </span></code></pre></div>
<div class="sourceCode" id="cb31"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">metrics.all</span><span class="op">[</span><span class="fl">1</span>,<span class="fl">1</span><span class="op">]</span></span></code></pre></div>
<div class="sourceCode" id="cb32"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">metrics.all</span><span class="op">[</span><span class="fl">2</span>,<span class="fl">1</span><span class="op">]</span></span></code></pre></div>
</div>
<div class="section level2">
<h2 id="varimppred">varimpPred()<a class="anchor" aria-label="anchor" href="#varimppred"></a>
</h2>
<p><code><a href="../reference/varimpPred.html">varimpPred()</a></code> returns classes probabilities and variables
importance plots and tables.</p>
<div class="sourceCode" id="cb33"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Calculate variable importance for classification model</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/utils/data.html" class="external-link">data</a></span><span class="op">(</span><span class="st">"septoriaDurumWC"</span><span class="op">)</span></span>
<span></span>
<span><span class="va">septoriaDurumWC</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/as.data.frame.html" class="external-link">as.data.frame</a></span><span class="op">(</span><span class="va">septoriaDurumWC</span><span class="op">)</span></span>
<span></span>
<span><span class="va">knn.varimp</span><span class="op">&lt;-</span> <span class="fu"><a href="../reference/varimpPred.html">varimpPred</a></span><span class="op">(</span>newdata <span class="op">=</span> <span class="va">knn.ST_S</span><span class="op">$</span><span class="va">`Test Data`</span>,</span>
<span>                                      y<span class="op">=</span><span class="st">'ST_S'</span>,</span>
<span>                                      model <span class="op">=</span> <span class="va">knn.ST_S</span><span class="op">$</span><span class="va">Tuning</span>,</span>
<span>                                      positive <span class="op">=</span> <span class="st">"R"</span>,</span>
<span>                                      auc <span class="op">=</span> <span class="cn">TRUE</span>, predict <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span></span>
<span></span>
<span><span class="va">knn.varimp</span></span></code></pre></div>
<div class="sourceCode" id="cb34"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># # Calculate variable importance and obtain class probabilities</span></span>
<span><span class="co"># # with highest measure</span></span>
<span><span class="co">#  testdata &lt;- rf.DHE_class$`Test Data`</span></span>
<span><span class="co"># # </span></span>
<span><span class="co"># # # Obtain variable importance plot for only first 20 variables</span></span>
<span><span class="co"># rf.varimp &lt;- icardaFIGSr::varimpPred(newdata = testdata,</span></span>
<span><span class="co">#                                      y = 'DHE_Class',</span></span>
<span><span class="co">#                             positive = 'Cl_1', </span></span>
<span><span class="co">#                             model = rf.DHE_Class$Tuning, # can also use Training object</span></span>
<span><span class="co">#                             ROC = TRUE, predict = TRUE, top = 20)</span></span>
<span><span class="co"># # # Check results</span></span>
<span><span class="co">#  rf.varimp</span></span></code></pre></div>
</div>
  </main><aside class="col-md-3"><nav id="toc" aria-label="Table of contents"><h2>On this page</h2>
    </nav></aside>
</div>



    <footer><div class="pkgdown-footer-left">
  <p>Developed by Khadija Aziz, Zakaria Kehel, Bancy Ngatia, Khadija Aouzal, Chafik Analy.</p>
</div>

<div class="pkgdown-footer-right">
  <p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.1.1.</p>
</div>

    </footer>
</div>





  </body>
</html>
