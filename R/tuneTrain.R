#' @title Splitting the Data, Tuning and Training the models, and making predictions
#' @description Automatic function for tuning and training data, it returns a list containing a model objects, data frame and plot.
#' @param data object of class "data.frame" with target variable and predictor variables.
#' @param y character. Target variable.
#' @param p numeric. Proportion of data to be used for training. Default: 0.7
#' @param method character. Type of model to use for classification or regression.
#' @param Length integer. Number of values to output for each tuning parameter. If \code{search = "random"} is passed to \code{\link[caret]{trainControl}} through \code{...}, this becomes the maximum number of tuning parameter combinations that are generated by the random search. Default: 10.
#' @param control character. Resampling method to use. Choices include: "boot", "boot632", "optimism_boot", "boot_all", "cv", "repeatedcv", "LOOCV", "LGOCV", "none", "oob", timeslice, "adaptive_cv", "adaptive_boot", or "adaptive_LGOCV". Default: "repeatedcv". See \code{\link[caret]{train}} for specific details on the resampling methods.
#' @param number integer. Number of cross-validation folds or number of resampling iterations. Default: 10.
#' @param repeats integer. Number of folds for repeated k-fold cross-validation if "repeatedcv" is chosen as the resampling method in \code{control}. Default: 10.
#' @param summary expression. Computes performance metrics across resamples. For numeric \code{y}, the mean squared error and R-squared are calculated. For factor \code{y}, the overall accuracy and Kappa are calculated. See \code{\link[caret]{trainControl}} and \code{\link[caret]{defaultSummary}} for details on specification and summary options. Default: multiClassSummary.
#' @param process character. Defines the pre-processing transformation of predictor variables to be done. Options are: "BoxCox", "YeoJohnson", "expoTrans", "center", "scale", "range", "knnImpute", "bagImpute", "medianImpute", "pca", "ica", or "spatialSign". See \code{\link[caret]{preProcess}} for specific details on each pre-processing transformation. Default: c('center', 'scale').
#' @param parallelComputing logical. indicates whether to also use the parallel processing. Default: False
#' @param ... additional arguments to be passed to \code{createDataPartition}, \code{trainControl} and \code{train} functions in the package \code{caret}.
#' @param imbalanceMethod Method for handling imbalanced data ("up", "down"). Default: NULL.
#' @param imbalanceThreshold Threshold to determine if data is imbalanced (numeric between 0 and 0.4). Default: 0.2.
#' @param classProbs a logical (default : TRUE assuming classification task). It should be set to FALSE for regression tasks in conjunction with \code{summary} argument set to "defaultSummary". See examples for more details.

#' @return A list object with results from tuning and training the model selected in \code{method}, together with predictions and class probabilities. The training and test data sets obtained from splitting the data are also returned in addition to model quality metrics and preformance plots.
#'
#' If \code{y} is factor, class probabilities are calculated for each class. If \code{y} is numeric, predicted values are calculated.
#'
#' A ROC curve is created if \code{y} is factor. Otherwise, a plot of residuals versus predicted values is created if \code{y} is numeric.
#'
#' \code{tuneTrain} relies on packages \code{caret}, \code{ggplot2} and \code{pROC} to perform the modelling and plotting.
#' @details Types of classification and regression models available for use with \code{tuneTrain} can be found using \code{names(getModelInfo())}. The results given depend on the type of model used.
#'
#' In addition to Model object, Model quality, Tuning and training datasets,For classification models, class probabilities and ROC curve are given in the results. For regression models, Variable importance, predictions and residuals versus predicted plot are given. \code{y} should be converted to either factor if performing classification or numeric if performing regression before specifying it in \code{tuneTrain}.
#'
#' @author Zakaria Kehel, Bancy Ngatia, Khadija Aziz, Chafik Analy
#' @examples
#' \dontrun{
#' ## Reading local test datasets
#' 
#' data("DurumWheatDHEWC")
#' data("BarleyRNOWC")
#' data("septoriaDurumWC")
#' 
#' ## Binary classification of ST_S with balanced data
#' rf.ST_S <- tuneTrain(data = as.data.frame(septoriaDurumWC),
#'                      y =  'ST_S',
#'                      method = 'rf',
#'                      summary = multiClassSummary,
#'                      repeats = 3,
#'                      classProbs = TRUE)
#' 
#' 
#' ## Binary classification of RNO with imbalanced data
#' knn.RNO <- tuneTrain(data = BarleyRNOWC,
#'                      y =  'RNO',
#'                      method = 'knn',
#'                      summary = multiClassSummary,
#'                      imbalanceMethod ="up",
#'                      imbalanceThreshold = 0.2,
#'                      process = c("scale","center"),
#'                      classProbs = TRUE,
#'                      repeats = 3)
#'                       
#' ## Regression of DHE
#' svm.DHE <- tuneTrain(data = DurumWheatDHEWC,
#'                      y =  'DHE',
#'                      method = 'knn',
#'                      summary = defaultSummary,
#'                      classProbs = FALSE,
#'                      repeats = 3)
#' 
#' 
#' ## Multiclass classification of DHE Classes with imbalanced data
#' 
#' # Create DHE Classes from DurumWheatDHEWC dataset
#' 
#' DurumWheatDHEWC$DHE_Class <- ifelse(DurumWheatDHEWC$DHE <=172, "1", 
#'                                ifelse(DurumWheatDHEWC$DHE <= 180, "2", 
#'                                       "3"))
#'                                        
#' DurumWheatDHEWC$DHE_Class <- factor(DurumWheatDHEWC$DHE_Class)
#'
#' Run Classification
#' rf.DHE.Classes <- tuneTrain(data = DurumWheatDHEWC,
#'                              y =  'DHE_Class',
#'                              method = 'rf',
#'                              summary = multiClassSummary,
#'                              imbalanceMethod ="up",
#'                              imbalanceThreshold = 0.2,
#'                              classProbs = TRUE,
#'                              repeats = 3)
#' 
#'  }
#' @seealso
#'  \code{\link[caret]{createDataPartition}},
#'  \code{\link[caret]{trainControl}},
#'  \code{\link[caret]{train}},
#'  \code{\link[caret]{predict.train}},
#'  \code{\link[ggplot2]{ggplot}},
#'  \code{\link[pROC]{auc}},
#'  \code{\link[pROC]{roc}},
#'  \code{\link[pROC]{ggroc}},
#' @name tuneTrain
#' @importFrom caret createDataPartition trainControl train predict.train
#' @importFrom utils stack
#' @importFrom ggplot2 ggplot aes geom_histogram theme_bw scale_colour_brewer scale_fill_brewer labs coord_equal annotate geom_point
#' @importFrom stats resid predict
#' @importFrom pROC auc roc ggroc
#' @details Imbalance handling methods "up" and "down" use ,respectively, upsample() and downsample() from caret subsampling methods.
#' @export

tuneTrain <- function(data, y, p = 0.7,
                      method = "knn",
                      imbalanceMethod = NULL, # Imbalanced data handling method
                      imbalanceThreshold = 0.2, # Threshold for imbalance handling
                      parallelComputing = FALSE,
                      control = "repeatedcv",
                      Length =10,
                      number = 10, repeats = 10,
                      process = c('center', 'scale'),
                      summary = multiClassSummary,
                      classProbs = FALSE, ...) {
  
  # Validate imbalanceThreshold
  if (!classProbs && is.null(imbalanceMethod) && (imbalanceThreshold < 0 || imbalanceThreshold > 0.4)) {
    stop("Error: ImbalanceThreshold must be between 0 and 0.4. No Data imbalance handling is applied.")
  } 
  
  # Ensure target variable exists in the dataset
  if (!y %in% names(data)) {
    stop("Error: Target variable not found in the dataset.")
  }
  
  # Set a seed for reproducibility
  set.seed(1234)
  
  # Split predictors and target variable
  x <- data[which(colnames(data) != y)]
  yvec <- data[[y]]
  
  # Split data into training and test sets
  trainIndex <- caret::createDataPartition(y = yvec, p = p, list = FALSE)
  data.train <- data[trainIndex, ]
  data.test <- data[-trainIndex, ]
  
  # Class probability conversion if required
  if (classProbs && all(unique(data.train[[y]]) %in% 1:9)) {
    data.train[[y]] <- as.factor(paste("Cl", data.train[[y]], sep = "_"))
    data.test[[y]] <- as.factor(paste("Cl", data.test[[y]], sep = "_"))
  }
  
  # Filter necessary columns for trainx and testx after preprocessing
  trainx <- data.train[, colnames(data.train) %in% colnames(x), drop = FALSE]
  testx <- data.test[, colnames(data.test) %in% colnames(x), drop = FALSE]
  
  # Extract target variable for training and test sets
  trainy <- data.train[[y]]
  testy <- data.test[[y]]
  
  # Parallel computing initialization if enabled
  if (parallelComputing) {
    cores <- parallel::detectCores()
    cls <-  parallel::makeCluster(max(1, cores - 1)) # Reserve at least 1 core
    doParallel::registerDoParallel(cls)
    
    on.exit({
      if (exists("cls")) {
        parallel::stopCluster(cls) 
      }
    }, add = TRUE)
  }
  
  # Determine subsampling method based on class probabilities and imbalance method
  subsampling <- if (classProbs && !is.null(imbalanceMethod)) {
    imbalanceMethod
  } else {
    NULL
  }
  
  ctrl = caret::trainControl(method = control,
                             number = number, 
                             repeats = repeats,
                             sampling = subsampling,
                             summaryFunction = summary, 
                             classProbs = classProbs)
  
  if (method == "treebag") {
    tune.mod = caret::train(trainx, trainy, method = method, 
                            tuneLength = Length, trControl = ctrl, 
                            preProcess=process)
    train.mod <- tune.mod
    print(train.mod)
  }
  
  else if (method == "nnet") {
    # Initial training to get the best tune parameters
    tune.mod <- caret::train(
      trainx, trainy, method = method,
      tuneLength = Length, trControl = ctrl, trace = FALSE,
      preProcess = process
    )
    print(tune.mod)
    
    # Extract the best size parameter
    size <- tune.mod[["bestTune"]][["size"]]
    
    # Dynamically calculate bounds based on predictors and rows
    num_predictors <- ncol(trainx)
    num_rows <- nrow(trainx)
    min_size <- 1
    max_size <- min(num_predictors, floor(num_rows / 10)) # Dynamic maximum size
    
    # Ensure seqStart and seqStop are within valid bounds
    seqStart <- max(min_size, size - 1)
    seqStop <- min(max_size, size + 1)
    seqInt <- 1
    
    # Default .decay range and steps
    decay_start <- 10^-5
    decay_stop <- 10^-1
    decay_steps <- 5
    decay_values <- 10^seq(log10(decay_start), log10(decay_stop), length.out = decay_steps)
    
    # Generate the tuning grid
    tuneGrid <- expand.grid(
      .size = seq(seqStart, seqStop, by = seqInt),
      .decay = decay_values
    )
    
    # Train control for the second model
    ctrl2 <- caret::trainControl(
      method = control, number = number,
      repeats = repeats, classProbs = classProbs,
      sampling = subsampling,
      summaryFunction = summary
    )
    
    # Final training with refined tuning grid
    train.mod <- caret::train(
      trainx, trainy, method = method,
      tuneGrid = tuneGrid, tuneLength = Length,
      preProcess = process, trControl = ctrl2, trace = FALSE
    )
    print(train.mod)
  }
  else {
    tune.mod = caret::train(trainx, trainy, method = method,
                            tuneLength = Length, preProcess=process,
                            trControl = ctrl)
    print(tune.mod)
    
    if (method == "knn") {
      k <- tune.mod[["bestTune"]][["k"]]
      
      if (k - 2 <= 0) {
        seqStart <- k
      }
      else {
        seqStart <- k - 2
      }
      seqStop <- k + 2
      seqInt <- 1
      tuneGrid <- expand.grid(.k = seq(seqStart, seqStop, by=seqInt))
    }
    else if (method == "rf") {
      mtry <- tune.mod[["bestTune"]][["mtry"]]
      
      # Ensure seqStart and seqStop are within valid range
      num_predictors <- ncol(x) # Number of predictors in the dataset
      seqStart <- max(1, mtry - 2) # Ensure seqStart is at least 1
      seqStop <- min(num_predictors, mtry + 2) # Ensure seqStop does not exceed the number of predictors
      seqInt <- 1
      
      # Generate the tuning grid
      tuneGrid <- expand.grid(.mtry = seq(seqStart, seqStop, by = seqInt))
    }
    else if (method == "svmLinear2") {
      cost <- tune.mod[["bestTune"]][["cost"]]
      
      if (cost - 0.25 <= 0 || cost - 0.5 <= 0 || cost - 0.75 <= 0 || cost - 1 <= 0) {
        seqStart <- cost
      }
      else {
        seqStart <- cost - 1
      }
      seqStop <- cost + 1
      seqInt <- 0.25
      tuneGrid <- expand.grid(.cost = seq(seqStart, seqStop, by=seqInt))
      
    } 
    
    ctrl2 = caret::trainControl(method = control, number = number, 
                                repeats = repeats, classProbs = classProbs, 
                                sampling = subsampling,
                                summaryFunction = summary)
    
    train.mod = caret::train(trainx, trainy, method, 
                             tuneGrid = tuneGrid, tuneLength = Length,
                             preProcess=process,
                             trControl = ctrl2)
    print(train.mod)
  }
  
  
  ## For binary classification
  if (is.factor(data[[y]]) && length(unique(data[[y]])) == 2) {
    
    # Predict probabilities
    prob.mod <- as.data.frame(caret::predict.train(train.mod, testx, type = "prob"))
    
    # Generate histogram for class probabilities
    prob.newdf <- utils::stack(prob.mod)
    colnames(prob.newdf) <- c("Probability", "Class")
    prob.hist <- ggplot2::ggplot(prob.newdf, aes(x = Probability, group=Class, colour = Class, fill = Class)) +
      ggplot2::geom_histogram(alpha = 0.4, position = "identity", binwidth = 0.1) + 
      ggplot2::theme_bw() + 
      ggplot2::scale_colour_brewer(palette = "Dark2") + 
      ggplot2::scale_fill_brewer(palette = "Dark2") + 
      ggplot2::facet_wrap(~Class) +
      ggplot2::labs(y = "Count")
    
    # Model Evaluation
    predictions <- stats::predict(train.mod, newdata = testx)
    confMatrix <- caret::confusionMatrix(predictions, testy) 
    
    # Calculate ROC curve and AUC
    roc_curve <- pROC::roc(response = testy, predictor = prob.mod[,2])
    auc_value <- pROC::auc(roc_curve)
    
    # Plot ROC curve
    roc_plot <- pROC::ggroc(roc_curve)
    roc_plot <- roc_plot + ggplot2::ggtitle(sprintf("ROC Curve (AUC = %.2f)", auc_value))
    
    
    # Return results
    results = list(
      Tuning = tune.mod, 
      Training = train.mod, 
      `Model quality` = confMatrix,
      Variableimportance =plot(varImp(train.mod)),
      ROC_Plot = roc_plot,
      `Class Probabilities` = prob.mod, 
      `Class Probabilities Plot` = prob.hist,
      `Training Data` = data.train, 
      `Test Data` = data.test
    )
    return(results)
  }
  
  # For multiclass classification
  if (is.factor(data[[y]]) && length(unique(data[[y]])) > 2) {
    
    prob.mod <- stats::predict(train.mod, testx, type = "prob")
    
    # Check for missing values in probabilities, which can indicate issues during training
    if (any(is.na(prob.mod))) {
      warning("Missing values found in predicted probabilities.")
    }
    
    # Multiclass ROC analysis (one-vs-all approach)
    roc_results <- list()
    auc_values <- list()
    class_names <- colnames(prob.mod)
    
    # Generate ROC plots and calculate AUC for each class (one-vs-all)
    roc_plots <- list()
    for (class_name in class_names) {
      # Create a one-vs-all response variable
      one_vs_all_response <- ifelse(testy == class_name, class_name, paste0("Not_", class_name))
      
      # Generate the ROC curve for this class
      one_vs_all <- pROC::roc(
        one_vs_all_response,
        prob.mod[, class_name],
        levels = c(paste0("Not_", class_name), class_name), # Ensure binary levels
        direction = "<"
      )
      
      # Calculate AUC
      auc <- pROC::auc(one_vs_all)
      auc_values[[class_name]] <- auc
      
      # Create the ROC plot
      plot_title <- sprintf("ROC Curve: %s vs All (AUC = %.2f)", class_name, auc)
      roc_plots[[class_name]] <- pROC::ggroc(one_vs_all) +
        ggplot2::ggtitle(plot_title) +
        ggplot2::theme_minimal() +
        ggplot2::theme(plot.title = ggplot2::element_text(size = 10, face = "bold"))
      
      # Save ROC result
      roc_results[[class_name]] <- one_vs_all
    }
    
    # Generating histograms for class probabilities
    prob.newdf <- utils::stack(as.data.frame(prob.mod))
    colnames(prob.newdf) <- c("Probability", "Class")
    prob.hist <- ggplot2::ggplot(prob.newdf, ggplot2::aes(x = Probability, fill = Class, color = Class)) +
      ggplot2::geom_histogram(alpha = 0.4, position = "identity", binwidth = 0.05) +
      ggplot2::theme_bw() +
      ggplot2::scale_colour_brewer(palette = "Paired") +
      ggplot2::scale_fill_brewer(palette = "Paired") +
      ggplot2::facet_wrap(~ Class) +
      ggplot2::labs(y = "Count")
    
    # Confusion matrix
    predictions <- stats::predict(train.mod, newdata = testx)
    confMatrix <- caret::confusionMatrix(predictions, testy)
    
    # Return a list of all evaluation results
    results <- list(
      Tuning = tune.mod,
      Training = train.mod,
      `Model quality` = confMatrix,
      Variableimportance = plot(varImp(train.mod)),
      ROC_Results = roc_results,
      AUC_Values = auc_values,
      ROC_Plots = roc_plots,
      Probabilities_Plot = prob.hist,
      Predictions = predictions,
      `Training Data` = data.train,
      `Test Data` = data.test
    )
    
    return(results)
  }
  
  ## For regression  
  else if(is.numeric(data[[y]])) {
    
    pred.mod = caret::predict.train(train.mod, newdata = testx)
    # Calculate residuals against the test set
    resids = testy - pred.mod
    Actual = testy
    # Prepare residuals for plotting
    respred.df = data.frame(Residuals = resids, Predicted = pred.mod)
    
    # Prepare predictions for plotting
    actual.vs.predicted.df = data.frame(Actual = Actual, Predicted = pred.mod)
    
    ## Model Quality metrics
    Quality_metrics <-caret::postResample(pred = actual.vs.predicted.df$Predicted,
                                    obs = actual.vs.predicted.df$Actual)
    
    # Residulas plots
    respred.plot = ggplot2::ggplot(data=respred.df, ggplot2::aes(x = Predicted, y = Residuals)) +
      ggplot2::geom_point() +  
      ggplot2::theme_bw() +
      ggplot2::labs(x = "Predicted", y = "Residuals")
    
    # Plotting Predicted vs Actual values
    actual.vs.predicted.plot = ggplot2::ggplot(data=actual.vs.predicted.df, ggplot2::aes(x = Actual, y = Predicted)) +
      ggplot2::geom_point() +
      ggplot2::geom_smooth(method = "loess")+
      ggplot2::theme_bw() +
      ggplot2::labs(x = "Actual", y = "Predicted", title = "Predicted vs Actual Values")
    
    # Get Variable importance
    VarImportance <- plot(caret::varImp(train.mod))
    
    # Gather results
    results = list(Tuning = tune.mod, 
                   Training = train.mod, 
                   Predictions = pred.mod, 
                   `Residuals Vs. Predicted Plot` = respred.plot, 
                   `Predicted vs Actual Plot` = actual.vs.predicted.plot,
                   Quality_metrics = Quality_metrics,
                   VariableImportance = VarImportance,
                   `Training Data` = data.train, 
                   `Test Data` = data.test)
    return(results)
  }
}
